---
title: "Gradient descent"
format:
  html:
    code-fold: true
    default-image-extension: svg
  pdf:
    default-image-extension: pdf
jupyter: python3
---

## Introduction

Moving forward with the groundwork from the previous section on automatic differentiation, we'll dive into how this enables a particular learning procedure called *gradient descent*. We'll explore what that means, then apply this to a few different scenarios to demonstrate its utility, along with highlighting some more intelligent versions that have arisen as a by-product of the recent deep learning revolution. Importantly, this will also set the stage to show the main mechanism for how neural networks are able to train.

Say we have a ball on a hill, and we wanted to roll it down. How would you go about this task? Well, I imagine you'd probably just give it some initial kick, and then let gravity do the rest. Alright then -- let's take away gravity. What then? Look for the bottom of the hill and try to push it there? But what if you can't see the hill itself? This is the situation we find ourself in when trying to optimize a workflow. We're able to see what point we're at during optimization, but we don't know where the bottom of the hill is (minimum of our objective), or what the surrounding landscape looks like (could be very expensive to scan over the space).

Ah, but here's a trick: if we can see where we are at one point on the hill, we can determine the way the slope goes that we're standing on. If the slope points up, we'll want to push the ball in the opposite direction to go down. Then, we're guaranteed to at least get *nearer* the bottom of the hill. But by how much do we push the ball?

If the ball is close to the bottom of the hill, you could imagine that the ground starts getting flatter, which would make the magnitude of the slope less. Likewise, if the slope is steep, we know we're probably not at the bottom yet, so we should move a reasonable amount. We can then just move *proportionally to the magnitude of the slope.*

To write this out in equation form: given the horizontal position on the hill $x_i$, and the vertical position $y_i$, we propose to move to a new point $x_{i+1}$ in proportion to the slope at the point that we're on, i.e.

$$
x_{i+1} = x_i - \alpha \frac{\Delta y_i}{\Delta x_i}~,
$$

where $\alpha$ is the constant of proportionality that we're free to choose (also called the **learning rate**, since it modifies the step size we take), and $\Delta$ is a small change that we assume we can calculate. Take note of the minus sign -- this is us trying to move in the *opposite direction to the slope* at $x_i$.

Instead of assuming we can numerically calculate the small change $\Delta$, we actually already have a way to calculate a small change at a point from calculus: the derivative! So if we take $y$ as a function of $x$, then we can replace the deltas with the gradient of $y(x)$ evaluated at our current point $x_i$, leaving us with

$$
x_{i+1} = x_i - \alpha \frac{\partial y(x_i)}{\partial x_i}~.
$$ {#eq-grad-descent}

@eq-grad-descent is the equation for gradient descent, and is probably the most important equation in the whole thesis, since it enables all of the downstream applications I'll talk about later. It's also the mechanism that enables most artificial intelligence systems to learn.

To reframe this equation one more time through the lens of optimization: given an **objective function** $L$ with parameters $\varphi$ that defines a goal relative to data $d$ (so $L=L(\varphi, x)$), we can use *gradient descent* to update the parameters $\varphi$ such that we minimize the objective evaluated at $d$:

$$
\varphi_{i+1} = \varphi_i - \alpha \frac{\partial L(\varphi_i, d)}{\partial \varphi_i}~.
$$ {#eq-grad-descent}

We're now specifying the data $d$ because we want to give meaning to the vertical position in the hill: it's some quantity that's assessing the quality of our workflow with respect to some data from the real world $d$, given that we're using parameters $\varphi_i$. In practice this may be a small subset of the dataset that we draw at random, since we can't computationally do this update on the whole data set due to memory restrictions on the device we run this on.

A key point to highlight: this mechanism only works if $L$ is differentiable with respect to $\varphi$, otherwise we won't be able to calculate its gradient. This restriction may look tame for common objectives, which tend to involve simple algebraic expressions involving the output of a neural network, which is already a differentiable function. However, if we want to add domain knowledge to our loss function, we may end up discovering that not everything we want to calculate has a well-defined gradient. There are various ways to get around this, including the use of surrogate operations that are **relaxed** (jargon for differentiable) versions of their non-differentiable counterparts. We'll look at this in more detail when we study applications of this nature.

Let's look at an example of gradient descent in action.

### Example: maximum likelihood estimation

Say we have some example data drawn from a bi-modal probability distribution that's somewhat normal-looking, like in @fig-bidata. We may want to try and model this with a normal distribution, but how do we choose the parameters? We can fit them to the data using maximum likelihood estimation, discussed further in @sec-mle. The basic idea is that, given a probability distribution $p$ with parameters $\mu$, we want to calculate
$$ \hat{\mu} = \underset{\mu}{\mathrm{argmin}} (-2\ln p(x|\mu))~,$$

given some observed data $x$. In other words, we want to find the value of $\mu$ such that we minimize the negative log-likelihood. We can do this via gradient descent!

Using the update rule in @eq-grad-descent with $L$ as the negative log-likelihood and $(\mu, \sigma)$ playing the role of $\varphi$, we can run gradient descent for some number of steps until we reach a result that converges within some tolerance. We'll have to pick some initial value to start for each parameter -- here we use 1 for each. In the implementation, we're using the automatic differentiation framework JAX (@jax) to calculate the gradient of the objective (online viewers can expand the code block above the figure to see the details). This gives us a result in @fig-onenorm, which isn't particularly great in my opinion.


```{python}
#| label: fig-bidata
#| fig-cap: "Example data produced using two normal distributions in unequal amounts. One is centered on 3, and the other on 0, both with unit standard deviation."
import numpy as np
import jax
import jax.scipy as jsc
import jax.numpy as jnp
import copy

num_points = 10000

data = np.concatenate(
    (np.random.normal(size=num_points), np.random.normal(size=num_points // 2) + 3)
)

# want to minimize negative log likelihood
def loss(pars, data):
    mu, sigma = pars
    return jnp.mean(-jsc.stats.norm.logpdf(data, mu, sigma))


# gradient descent
def update_rule(pars, data, lr, loss):
    return pars - lr * jax.grad(loss)(pars, data)


def learning(loss, init_pars, num_steps=1000, tol=1e-6, lr=5e-2):
    pars = copy.deepcopy(init_pars)
    for i in range(num_steps):
        new_pars = update_rule(pars, data, lr, loss)
        if not np.allclose(new_pars, pars, atol=tol):
            pars = new_pars
        else:
            return new_pars
    print("max steps reached")
    return pars


pars = learning(loss, init_pars=jnp.array([1.0, 1.0]))

import matplotlib.pyplot as plt

subplot_settings = dict(figsize=[7, 3], dpi=200, tight_layout=True)
fig, ax = plt.subplots(**subplot_settings)

ax.hist(data, density=True)
ax.set_xlabel("x")
ax.set_ylabel("density");
```


```{python}
#| label: fig-onenorm
#| fig-cap: "A single normal distribution fitted to the data in @fig-bidata using gradient descent."

fig, ax = plt.subplots(**subplot_settings)

ax.hist(data, density=True)
x = np.linspace(jsc.stats.norm.ppf(0.01, *pars), jsc.stats.norm.ppf(0.99, *pars), 100)
ax.plot(
    x,
    jsc.stats.norm.pdf(x, *pars),
    label=f"Normal($\mu$={pars[0]:.3g}, $\sigma$={pars[1]:.3g})",
)
ax.set_xlabel("x")
ax.set_ylabel("density")
ax.set_title(f"negative log-likelihood = {loss(pars, data):.3g}")
ax.legend();
```

Pretending we didn't know that the data came from a mixture of normal distributions, we can make a more sophisticated model, e.g.

$$
p(x |\mu_1, \sigma_1, \mu_2, \sigma_2) = \frac{1}{2}\mathrm{Normal}(\mu_1, \sigma_1) + \frac{1}{2}\mathrm{Normal}(\mu_2, \sigma_2)~.
$$

We can then simultaneously optimize $\mu_1, \sigma_1, \mu_2, \sigma_2$ in exactly the same way, with no modification to the procedure other than using the new likelihood as the loss function. Doing this yields the distribution in @fig-twonorm. We can tell that the shape of the distribution is represented better here, which is also indicated by the lower negative log-likelihood than in the case of the single normal distribution. Since we forced the proportions of the mixture to be half and half, the lower second peak is modelled through a wider normal distribution to match the height of the second, smaller mode.

Interestingly, if we use 1 as the init for every parameter, we recover the solution from @fig-onenorm, so we have to make sure there's a little mutual variation in the starting values so that the gradients push the different $\mu$ and $\sigma$ values from different positions, allowing the second mode to be discovered. This demonstrates the behavior of gradient descent to move to some *local* minimum of the objective, and won't always converge to something optimal or intuitive.

```{python}
#| label: fig-twonorm
#| fig-cap: "A mixture of two normal distributions fitted to the data in @fig-bidata using gradient descent."

def mixture_pdf(data, pars):
    mu1, sigma1, mu2, sigma2 = pars
    return 0.5 * jsc.stats.norm.pdf(data, mu1, sigma1) + 0.5 * jsc.stats.norm.pdf(
        data, mu2, sigma2
    )


def new_loss(pars, data):
    return jnp.mean(-jnp.log(mixture_pdf(data, pars)))


new_pars = learning(
    new_loss, init_pars=jnp.array([1.0, 1.0, 2.0, 2.0]), lr=1e-1, tol=1e-4
)

fig, ax = plt.subplots(**subplot_settings)

ax.hist(data, density=True)
x = np.linspace(-3, 6, 100)
mu1, sigma1, mu2, sigma2 = new_pars
ax.plot(
    x,
    mixture_pdf(x, new_pars),
    label=f"0.5*Normal($\mu$={mu1:.2g}, $\sigma$={sigma1:.2g})"
    + f" + 0.5*Normal($\mu$={mu2:.2g}, $\sigma$={sigma2:.2g})",
)
ax.set_xlabel("x")
ax.set_ylabel("density")
ax.set_title(f"negative log-likelihood = {new_loss(new_pars, data):.3g}")
ax.legend(fontsize="small");
```
## Batching and mini-batching

In reality, we're not generally able to update our parameters using gradients computed over all our data at once, since that data may not fit on the device we're using to compute the program. We'd like to do this if we truly want to calculate the expectation of the gradient (in practice, the empirical mean) across all the training data we have, which has some convergence guarantees for arriving at a local minimum. Instead, we often split our data up into **batches**, which are fixed-size partitions of the data, typically randomly selected. This gives rise to the notion of the **batch size**, which is the size of each partition. We can then compute gradients using their values aggregated across each batch. The intuition is that we're still rolling somewhat down the hill, even if we're not following the exact trajectory defined by the entirety of the training data at once. This method doesn't come with the same convergence guarantees, but in practice will often converge well anyway, given the right hyperparameters like learning rate and initial position.

Now, despite this, the nomenclature used is to refer to the gradient over the whole dataset as "batch gradient descent", and the partitioning of data into batches as "minibatch gradient descent". I will keep referring to the minibatches as just batches though.

An extreme version of splitting up into batches is **stochastic gradient descent**, which is just setting the batch size to 1. This is much more stochastic as the name suggests, since it's updating the parameters based on the performance on each individual training example. The high variance of the gradients can often help when wanting to explore the loss landscape more, and may help in jumping to new local minima that batch gradient descent may ignore.

## Beyond standard gradient descent

### Momentum
### Adam

## How to pick the best parameters

### Early stopping

### k-fold validation
