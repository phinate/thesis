---
title: "Statistics in High-Energy Physics: Fundamentals"
format:
  html:
    code-fold: true
    default-image-extension: pdf
  pdf:
    default-image-extension: tex
jupyter: python3
---

```{python}

x = 2
```

```{python}

print(x)
```

## Likelihood

- Model is a data-generating story

## Inference

**Statistical inference** can be viewed as the inverse of the data generating process. Let's look at an example.

Say that the Lorax created the Earth. Moreover, say that he did so by rolling a six-sided die, and putting the result of the die into a World Generating Machine, which merely requires a single number as input. As a neutral party that cannot confirm the Lorax's roll, but can definitely observe the outcome, we may wonder: assuming we have a model of the World Generating Machine, what number did the Lorax roll to produce all that we see around us today? Moreover, can we factor in our prior suspicion that a Lorax rolled a die to select a number at random?

Put in a more general way: given a parametrized model of the world around us, potential suspicions about the parameters themeselves, and some data that could be described by that model, which values of the parameters could have produced the data? Moreover, is the model a good description of the data at all? Could another model have described the data more accurately?
These type of questions fall within the umbrella of statistical inference.

Let us add a layer of specificity to our questions. The key inquiries that are typically dealt with through inference are:

- **Point estimation**: What value(s) of my model parameters best describe the data?
- **Interval estimation**: How can I specify a plausible range for my model parameters based on the data they attempt to describe? (One can obtain a symbiotic relation between interval and point estimation by viewing a point estimate as the limit of shrinking intervals)
- **Hypothesis testing**: Given a model of the data generating process, to what extent does it describe the data when...
    - ...compared to another model with different form?
    - ...compared to the same model with different parameters?
    - ...not being compared to any particular alternative? (this has the particular name of the *goodness-of-fit test*)


These questions are where the schools of probability see their strongest divide; whether we can assign probabilities to facts -- e.g. what are the odds that neutrinos have masses -- fundamentally alters the kind of question we can ask of our model/data. For example, instead of a point estimate of the neutrino masses, a Bayesian procedure could infer their *distribution* given the data (and some prior beliefs about their values), also termed the *posterior distribution*. Similarly, Bayesian intervals can say "what are the chances the true parameter value lies in this interval" (which frequentist procedures *can not determine*), and hypothesis testing can evaluate the probability of the hypothesis itself.

However, these Bayesian procedures undertake the weight of the specification of a prior distribution for all of these quantities, which will (in the limited data regime, a.k.a. real life) strongly affect the result. Moreover, the frequentist procedures are indeed still useful for making exact, even if convoluted, statements about the model in the presence of observed data.

A brief aside: you'll notice that the common thread throughout these statements is the *model*. As like many statistics-adjacent HEP researchers before me, I will emphasize that *the likelihood model is the most important ingredient for any inference procedure*. In the limit of a well-designed model and sufficient data, even a room full of the most polarized statisticians, hopelessly resigned to their particular school of thought, will agree on basic statements about the validity of a hypothesis. Even failing this,a Bayesian would not ignore a $p$-value of 0.9, and a frequentist would likewise raise their eyebrows at a flat or sharp posterior with low prior sensitivity. But since they both care about the model, any and all effort that could be spent arguing over inferencial philosophy may likely be better placed in talking to a domain expert for the problems you care about.

But enough with the aside, or I'll be subject to the same criticism of misplaced effort. Let's review the different methods that implement answers to our questions of inference.

### Frequentist procedures

#### Point estimation

#### Confidence intervals

A significant part of my early PhD days was spent trying to understand the confidence interval. I will do my best to try and make you bang your head against the wall a little less than me (metaphorically, I hope).

A confidence interval is a statement about values of a model parameter $\mu$ for which the observed data is considered *"not extreme"*. Values of $\mu$ outside the interval are then those for which the observed data is *"extreme"*. We have a couple of ingredients to unpack here:
- How do we define a notion of "extreme" for the data?
- How do we construct the interval itself to satisfy this property (namely that the values of $\mu$ consistently treat $x$ as extreme or not)?

For the first point, there's a two-part answer: we need a way to order the data with a notion of extremity (rank it from least to most extreme), and then also determine the cutoff for which points are considered extreme or not. This cutoff isn't going to be by value, but by *proportion*; we'll place our yardstick such that all values of $x$ considered *not* extreme contain the majority of the probability (area under $p(x|\mu)$). How much? That's known as the **confidence level** ($\mathrm{C.L.}$). Values considered *extreme* then occupy the other $1-\mathrm{C.L.}$ of the probability. We then determine those values of $\mu$ which produce distributions that satisfy this requirement *when the yardstick is the observed data $x_0$*. We'll see some examples of this below.

For 1-D $x$, we look at some simple orderings:
- *Descending*: large $x$ is not considered extreme, and small $x$ is. The corresponding confidence interval is $[\mu_{\mathrm{lower}}, +\infty]$, where $\mu_{\mathrm{lower}}$ is known as a **lower limit** on $\mu$.
- *Ascending*: small $x$ is now not extreme, but large $x$ is. The corresponding confidence interval is $[-\infty, \mu_{\mathrm{upper}}]$, where $\mu_{\mathrm{upper}}$ is known as an **upper limit** on $\mu$.

We can also look at a *central* ordering, which produces an interval that can be constructed at a given confidence level $\mathrm{C.L.}$ through calculating a lower and upper limit $[\mu_{\mathrm{lower}}, \mu_{\mathrm{upper}}]$, each with a confidence level of $1-(1-\mathrm{C.L.})/2$ (which guarantees that the central interval contains $\mathrm{C.L.}$ of the probability).


Despite that a confidence interval is in values of $\mu$, it's an inherently *data-dependent construct* -- you'll get a different confidence interval for different observed data $x_0$. The confidence level, then, is a property of the set of intervals that would be constructed over many experiments: if we sampled $x$ from the true distribution many times with many experiments, and constructed a confidence interval using each sample with confidence level of 95%, then 95% of those intervals would contain (or **cover**) the true value of $\mu$. The collection of these confidence intervals is called a **confidence set**, and we say that it has 95% **coverage**. (It's worth noting that the coverage is something that is totally dependent on the method of interval construction, and it should be checked in practice).

This may ring some alarm bells -- we only have one interval in practice, right? Isn't there a chance that it falls in the 5% that don't cover the true, unknown value of $\mu$, and thereby *contradicting the distribution that produced the data*?

In short, yes. But that doesn't mean confidence intervals are useless -- a single 95% interval is overwhelmingly more likely to contain the true parameter value than not. Remember: we're always going to have inherent uncertainty on the statements we make on unknown parameter values -- this uncertainty is expressed as a distribution for Bayesian methods, and as a statement about sampling properties (results in the limit of many repeated experiments) for frequentist methods.

#### Hypothesis testing

### Bayesian procedures

#### Posterior estimation

#### Credible intervals

#### Hypothesis testing
