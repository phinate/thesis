---
title: "Statistics in High-Energy Physics: Fundamentals"
format:
  html:
    code-fold: true
    default-image-extension: svg
  pdf:
    default-image-extension: pdf
jupyter: python3
---


## Likelihood

- Model is a data-generating story

## Inference

**Statistical inference** can be viewed as the inverse of the data generating process. Let's look at an example.

Say that the Lorax created the Earth. Moreover, say that he did so by rolling a six-sided die, and putting the result of the die into a World Generating Machine, which merely requires a single number as input. As a neutral party that cannot confirm the Lorax's roll, but can definitely observe the outcome, we may wonder: assuming we have a model of the World Generating Machine, what number did the Lorax roll to produce all that we see around us today? Moreover, can we factor in our prior suspicion that a Lorax rolled a die to select a number at random?

Put in a more general way: given a parametrized model of the world around us, potential suspicions about the parameters themeselves, and some data that could be described by that model, which values of the parameters could have produced the data? Moreover, is the model a good description of the data at all? Could another model have described the data more accurately?
These type of questions fall within the umbrella of statistical inference.

Let us add a layer of specificity to our questions. The key inquiries that are typically dealt with through inference are:

- **Point estimation**: What value(s) of my model parameters best describe the data?
- **Interval estimation**: How can I specify a plausible range for my model parameters based on the data they attempt to describe? (One can obtain a symbiotic relation between interval and point estimation by viewing a point estimate as the limit of shrinking intervals)
- **Hypothesis testing**: Given a model of the data generating process, to what extent does it describe the data when...
    - ...compared to another model with different form?
    - ...compared to the same model with different parameters?
    - ...not being compared to any particular alternative? (this has the particular name of the *goodness-of-fit test*)


These questions are where the schools of probability see their strongest divide; whether we can assign probabilities to facts -- e.g. what are the odds that neutrinos have masses -- fundamentally alters the kind of question we can ask of our model/data. For example, instead of a point estimate of the neutrino masses, a Bayesian procedure could infer their *distribution* given the data (and some prior beliefs about their values), also termed the *posterior distribution*. Similarly, Bayesian intervals can say "what are the chances the true parameter value lies in this interval" (which frequentist procedures *can not determine*), and hypothesis testing can evaluate the probability of the hypothesis itself.

However, these Bayesian procedures undertake the weight of the specification of a prior distribution for all of these quantities, which will (in the limited data regime, a.k.a. real life) strongly affect the result. Moreover, the frequentist procedures are indeed still useful for making exact, even if convoluted, statements about the model in the presence of observed data.

A brief aside: you'll notice that the common thread throughout these statements is the *model*. As like many statistics-adjacent HEP researchers before me, I will emphasize that *the likelihood model is the most important ingredient for any inference procedure*. In the limit of a well-designed model and sufficient data, even a room full of the most polarized statisticians, hopelessly resigned to their particular school of thought, will agree on basic statements about the validity of a hypothesis. Even failing this,a Bayesian would not ignore a $p$-value of 0.9, and a frequentist would likewise raise their eyebrows at a flat or sharp posterior with low prior sensitivity. But since they both care about the model, any and all effort that could be spent arguing over inferencial philosophy may likely be better placed in talking to a domain expert for the problems you care about.

But enough with the aside, or I'll be subject to the same criticism of misplaced effort. Let's review the different methods that implement answers to our questions of inference.

### Confidence intervals

A significant part of my early PhD days was spent trying to understand the confidence interval. I will do my best to try and make you bang your head against the wall a little less than me (metaphorically, I hope).

We begin with some statistic of the observed data $x$. A confidence interval is then a statement about values of a model parameter $\mu$ for which the observed data is considered *"not extreme"*. Values of $\mu$ outside the interval are then those for which the observed data is *"extreme"*. We have a couple of ingredients to unpack here:

- How do we define a notion of "extreme" for the data?
- How do we construct the interval itself to satisfy this property (namely that the values of $\mu$ consistently treat $x$ as extreme or not)?

For the first point, there's a two-part answer: we need a way to order the data with a notion of extremity (rank it from least to most extreme), and then also determine the cutoff for which points are considered extreme or not. This cutoff isn't going to be by value, but by *proportion*; we'll place our yardstick such that all values of $x$ considered *not* extreme contain the majority of the probability (area under $p(x|\mu)$). How much? That's known as the **confidence level** ($\mathrm{C.L.}$). Values considered *extreme* then occupy the other $1-\mathrm{C.L.}$ of the probability. We then determine those values of $\mu$ which produce distributions that satisfy this requirement *when the yardstick is in the same place as the observed data $x_0$*. We'll see some examples of this below.

For 1-D $x$, we look at some simple orderings:
- *Descending*: large $x$ is not considered extreme, and small $x$ is. The corresponding confidence interval is $[\mu_{\mathrm{lower}}, +\infty]$, where $\mu_{\mathrm{lower}}$ is known as a **lower limit** on $\mu$.
- *Ascending*: small $x$ is now not extreme, but large $x$ is. The corresponding confidence interval is $[-\infty, \mu_{\mathrm{upper}}]$, where $\mu_{\mathrm{upper}}$ is known as an **upper limit** on $\mu$.

We can also look at a *central* ordering, which produces an interval that can be constructed at a given confidence level $\mathrm{C.L.}$ through calculating a lower and upper limit $[\mu_{\mathrm{lower}}, \mu_{\mathrm{upper}}]$, each with a confidence level of $1-(1-\mathrm{C.L.})/2$ (which guarantees that the central interval contains $\mathrm{C.L.}$ of the probability).

Let's make this concrete with an example: we'll take the same distribution studied in [@bob] (Section 6.4), where we have a normal distribution with width parametrized as 1/5th of the mean:

$$x \sim \mathrm{Normal}(\mu, \frac{\mu}{5}).$$

From here, we can examime the pdf (at $\mu=10$ arbitrarily). We can also view the likelihood if we say we observed data $x_0=10$. Both are shown in [@fig-density].

```{python}
#| label: fig-density
#| fig-cap: "Probability density function and likelihood for different assumed values of $x$ and $\\mu$."
import jax
import jax.numpy as jnp
import jax.scipy as jsc
from jaxopt import OptaxSolver
from optax import adam
import matplotlib.pyplot as plt

subplot_settings = dict(figsize=[7, 3], dpi=150, tight_layout=True)

def pdf(x, mu):
    return jsc.stats.norm.pdf(x, mu, 0.2 * mu)

mu = 10
x0 = 10

grid = jnp.linspace(0, 20, 100)

fig, axs = plt.subplots(1, 2, **subplot_settings)

ax = axs[0]
ax.plot(grid, pdf(grid, mu))
ax.axvline(x0, label="$x_0$", linestyle="dotted", color="C9")
ax.set_xlabel("x")
ax.set_ylabel("$p(x|\mu=10)$")
ax.legend()
ax.set_title("Probability density for $\mu =10$")
ax = axs[1]
ax.plot(grid, jax.vmap(pdf, in_axes=(None, 0))(x0, grid), color="C9")
ax.set_xlabel("$\mu$")
ax.set_ylabel("$p(x=x_0|\mu)$")
ax.set_title("Likelihood for $x =x_0 =10$");
```

Let's now use the aforementioned method to construct a central confidence interval for $\mu$, at say a confidence level of 68.3%. We then divide up this confidence level into the lower and upper limits -- each will be set with a confidence level of $1-(1-0.683)/2 = 84.15\%$, such that the extreme data occupies one minus that amount (15.95%) of the probability on each side of the central interval, giving us back the desired 68.3% between the upper and lower limits.

Finding these limits can be viewed as an optimization problem: we just need to choose $\mu$ such that the area under $p(x|\mu)$ is equal to 84.15% *below* $x_0$ for lower limits, and *above* $x_0$ for upper limits. We can frame this objective using the cumulative density function -- the cdf evaluated at $x_0$ must be equal to 0.843 for lower limits, and equal to 1-0.843 for upper limits. As this thesis focuses on methods relating to gradient descent, we can use this to find our central interval, by optimizing $\mu$ with respect to a mean-squared error loss between the cdf and the relevant probability share. Doing this for our example gives us the limits [8.332, 12.502] that form a central confidence interval for $\mu$.

```{python}
def central_interval(data, conf_level, inits):
    level = 1 - (1 - conf_level) / 2

    def solve(init, level):
        def loss(mu):
            return (level - jsc.stats.norm.cdf(data, mu, mu * 0.2)) ** 2

        solver = OptaxSolver(loss, adam(1e-3), tol=1e-8, maxiter=1e5)
        return solver.run(init).params

    lower = solve(inits[0], level)
    upper = solve(inits[1], 1 - level)

    return lower, upper


mu1, mu2 = central_interval(x0, 0.683, [5.0, 5.0])
```

We can visualize the probability density for each of $\mu_{\mathrm{lower}}$ and $\mu_{\mathrm{upper}}$, which should help cement the idea of capturing some amount of the probability up to $x_0$ based on an ordering of $x$:

```{python}
#| label: fig-uplow
#| fig-cap: "Probability density functions at both $\\mu_{\\mathrm{lower}}$ and $\\mu_{\\mathrm{upper}}$, showing that the enclosed area under the curve at $x_0$ equals the desired amount as dictated by the confidence level."
fig, axs = plt.subplots(1, 2, **subplot_settings, sharex=True, sharey=True)

ax = axs[0]
ax.plot(grid, pdf(grid, mu1))
ax.fill_between(
    grid[grid < x0],
    0,
    pdf(grid[grid < x0], mu1),
    hatch="//",
    edgecolor="C0",
    alpha=0.3,
    label="86% prob. (from below)",
)
ax.axvline(x0, label="$x_0$", linestyle="dotted", color="C9")
ax.set_xlabel("x")
ax.set_ylabel("$p(x|\mu)$")
ax.legend(loc="upper right")
ax.set_title(r"$\mu=\mu_{\mathrm{lower}} = $" + f"{mu1:.3g}")
ax = axs[1]
ax.plot(grid, pdf(grid, mu2))
ax.fill_between(
    grid[grid > x0],
    0,
    pdf(grid[grid > x0], mu2),
    hatch="//",
    edgecolor="C0",
    alpha=0.3,
    label="86% prob. (from above)",
)
ax.axvline(x0, label="$x_0$", linestyle="dotted", color="C9")
ax.set_xlabel("x")
# ax.set_ylabel("$p(x|\mu)$")
ax.legend(loc="upper right")
ax.set_title(r"$\mu=\mu_{\mathrm{upper}} = $" + f"{mu2:.3g}");
```

#### Neyman construction {-}

A different way to build confidence intervals (that generalizes to multiple dimensions) can be found in the form of the **Neyman construction**. Here, we start by constructing an *acceptance interval* for $x$. Given an ordering of $x$, a desired confidence level $\mathrm{C.L.}$, and a value of $\mu$, an acceptance interval for $x$ is defined such that the probability contained between $[x_1, x_2]$ is equal to $\mathrm{C.L.}$. In equation form:

$$ [x_1, x_2]~s.t.~P(x\in[x_1,x_2] | \mu) = \mathrm{C.L.} .$$

The endpoints $[x_1, x_2]$ are well defined because we decided on an ordering for $x$ (does not need to be central!), with any data outside these endpoints designated as "extreme" with respect to that ordering.

We can then draw the acceptance intervals for many values of $\mu$. This can be visualized in a plot of $\mu$ against $x$ as a set of horizontal lines in $x$ corresponding to the acceptance intervals. How do we turn this into an interval on values of $\mu$? We just draw a vertical line corresponding to our observed data $x_0$, and choose the values of $\mu$ that lie where the line intercepts the acceptance intervals for $x$. All of this is shown in [@fig-neyman].

```{python}
#| label: fig-neyman
#| fig-cap: "Neyman confidence band, showing both the overall envelop and explicit acceptance intervals in $x$. Also shows how one can then determine $\\mu_{\\mathrm{lower}}$ and $\\mu_{\\mathrm{upper}}$ at the intersection of the band and observed data $x_0$."
def central_acceptance_interval(mu, conf_level, inits):
    level = 1 - (1 - conf_level) / 2

    def solve(init, level):
        def loss(data):
            return (level - jsc.stats.norm.cdf(data, mu, mu * 0.2)) ** 2

        solver = OptaxSolver(loss, adam(1e-3), tol=1e-8, maxiter=1e5)
        return solver.run(init).params

    lower = solve(inits[0], level)
    upper = solve(inits[1], 1 - level)

    return lower, upper


def neyman_interval(mu_grid, conf_level, inits):
    central = jax.vmap(central_acceptance_interval, in_axes=(0, None, None))
    return central(mu_grid, conf_level, inits)

mu_grid = jnp.linspace(5,15, 50)
x1, x2 = neyman_interval(mu_grid, 0.683, [1.,1.])
fig, ax = plt.subplots(**subplot_settings)
ax.plot(x1, mu_grid, color='C0')
ax.plot(x2, mu_grid, color='C0')
ax.set_xlabel('x')
ax.set_ylabel('$\mu$')
ax.hlines(mu_grid, x1, x2, alpha=0.6, label="acceptance intervals @ C.L. = 68.3%")
ax.axvline(x0, label="$x_0$", linestyle="dotted", color="C9")
ax.annotate(r"$\mu_{\mathrm{upper}} = $" + f"{mu2:.3g}", xy=(10, mu2), xytext=(6, mu2),
            arrowprops=dict(arrowstyle="->"))
ax.annotate(r"$\mu_{\mathrm{lower}} = $" + f"{mu1:.3g}", xy=(10, mu1), xytext=(12, mu1),
            arrowprops=dict(arrowstyle="->"))
ax.set_title(r"Plot of the 68.3% central Neyman confidence band for $x\sim\mathrm{Normal}(\mu, \frac{\mu}{5})$")
ax.legend();
```

Even though the appearance of the plot in [@fig-neyman] makes it look so, it is completely *not required that $\mu$ be 1-D!* For example, $\mu$ could easily have been 2-D here, and then the confidence interval in 2-D could still be constructed by the points at which the observed data (which would be the plane defined by $x_0=10$) meets the acceptance intervals, even if this stretches out over other parameter dimensions. What happens if that multi-dimensional space becomes even larger, even if we don't care about some of the parameters? We'll look at new ordering principle for this.

#### Likelihood ratio ordering {-}

A fourth ordering principle for the data was proposed by Feldman & Cousins in [@FC], and is based on the likelihood ratio

$$ R(x, \mu) = \frac{p(x|\mu)}{p(x| \hat{\mu})} ,$$

where $\hat{\mu}$ represents the best-fit value of $\mu$. Commonly used in HEP, it appeals to intuition since there's then a correspondence between the notion of "extreme" and low probability density (relative to the maximum). Moreover, since it's a ratio, a change in variables from $x$ to $f(x)$ would lead to the Jacobians cancelling in $R$, making this ordering invariant to a change of metric. We can construct intervals ordered by $R$ through the Neyman method as above.

##### Profiling {-}

In the case where the likelihood parameters can be split into $\mu, \theta$, with $\theta$ representing nuisance parameters, we can make this interval construction more feasable by using the **profile likelihood ratio**:

$$ \lambda(x, \mu) = \frac{p\left(x|\mu,\hat{\hat{\theta}}(\mu)\right)}{p\left(x| \hat{\mu}, \hat{\theta}\right)},$$

where $\hat{\hat{\theta}}(\mu)$ represents the fitted value of $\theta$ when holding $\mu$ fixed. Doing this reduces the dimensionality of the parameter space to just those of which we're interested in (e.g. signal strength). However, coverage properties for intervals constructed in this way may vary, so need to have their sampling properties studied (see section 13 in [@bob]).

#### Coverage {-}

Despite that a confidence interval is in values of $\mu$, it's an inherently *data-dependent construct* -- you'll get a different confidence interval for different observed data $x_0$. The endpoints of the interval $[\mu_{\mathrm{lower}}, \mu_{\mathrm{upper}}]$ are consequently treated as random variables. The confidence level, then, is a property of the set of intervals that would be constructed over many experiments: if we sampled $x$ from the true distribution many times with many experiments, and constructed a confidence interval using each sample with confidence level of 95%, then 95% of those intervals would contain (or **cover**) the true value of $\mu$. The collection of these confidence intervals is called a **confidence set**, and we say that it has 95% **coverage**.

How do we know our confidence set has this property? Well, the Neyman confidence set has it by definition: if we think about just the single distribution at the true value $\mu_T$, the Neyman procedure creates an acceptance interval such that the probability between $[x_1, x_2]$ is equal to the confidence level (say 95%). If we then do many experiments (i.e. sample $x\sim p(x | \mu_T)$), we know that 95% of the time, the dotted line representing the data point will lie within the acceptance interval of $\mu_T$. Ah -- this is exactly the requirement for $\mu_T$ to be included in our confidence interval! It then follows that 95% of the *intervals* constructed will then include the true value of $\mu$, corresponding exactly to when the observed data lies within the acceptance interval for $\mu=\mu_T$.

This is all well and good, but may ring some alarm bells -- we only have one interval in practice, right? Isn't there a chance that we draw data such that the confidence interval falls in the 5% that don't cover the true, unknown value of $\mu$, and thereby *contradicting the distribution that produced the data point*?

In short, yes. But that doesn't mean confidence intervals are useless -- a single 95% interval is overwhelmingly more likely to contain the true parameter value than not. Remember: we're always going to have inherent uncertainty on the statements we make on unknown true parameter values -- this uncertainty is expressed as a probability distribution for Bayesian methods, and as a statement about sampling properties (results in the limit of many repeated experiments) for frequentist methods.

#### Extras {-}



### Frequentist hypothesis testing

Hypothesis testing gives us a framework to make qualitative statements about how extreme the observed data seems under a proposed theory, often in comparison to various alternatives. Note we use "extreme" in the same way as with confidence intervals, so we're going to need to establish an ordering principle for the data. Let's go into more detail.

Once again, we concern ourselves with a statistic of the data $x$ and model parameters $\mu$. Here we follow the conventions set out by Neyman (again) and Pearson in [@hyptests], and start with the notion of a **null** hypothesis $H_0$, which is the assumed default model for the data. One would often take this to be the Standard Model, for example. We then come up with an alternative hypothesis $H_1$. In what way can $H_0$ and $H_1$ differ? We could have:

- Two totally separate probability models $p_1(x | \mu_1)$ and $p_2(x | \mu_2)$, which can differ in number of parameters and functional form
- A set of hypotheses within a probability model; $p(x | \mu)$ induces a family of hypotheses parametrized by $\mu$, for instance, with any two values of $\mu$ potentially serving as $H_0$ and $H_1$

A hypothesis at a point (e.g. $p(x | \mu=0)$) is known as a **simple hypothesis**, and a set of hypotheses (e.g. a second, unspecified model $p_2(x | \mu)$, or $p(x | \mu\neq0)$) is known as a **composite hypothesis**. In HEP, we're commonly testing a point null of $H_0$ = Standard Model ($p(x | \mu=0)$) against a composite alternative of $H_1$ = Something Else ($p(x | \mu\neq0)$). This poses a challenge from an optimality perspective that we'll cover shortly.

So, how do we actually do the testing itself? We'll need an ordering principle, which is expressed as a **test statistic** of the data $t(x)$, to rank data from least to most extreme. From there, we come up with a cutoff value $t_c$ such that data with $t > t_c$ is seen as extreme. The proportion of the probability held by these extreme values is known as the **confidence level** or **test size** $\alpha$.

Under *which distribution* is this ordering and designation of $\alpha$ performed? We do this with the pdf of the test statistic under the null hypothesis, $p(t(x)|H_0)$. Importantly, this distribution may not be known to us, and we may have to estimate it by sampling many values of $x \sim p(x|H_0)$, then calculate $t(x)$ for all of them, and estimate the density of the result. Moreover, without this distribution, we don't know how to place our cutoff $t_c$ so it contains a proportion $\alpha$ of the data considered most extreme!

One more thing we can deduce from the above: our statements on how extreme the observed data appears are *fundamentally under the assumption that $H_0$ is true.* Keep this in mind!

 If the observed data $x_0$ falls in the extreme proportion of possible values, i.e. $t(x_0) > t_c$, we then say we *reject* $H_0$ compared to $H_1$. Note that this does *not mean we accept $H_1$!* For that, we'd have to look at decision theory, which involves establishing the notion of probabilities for $H_0$ and $H_1$, which is beyond the scope of this frequentist testing framework (a more detailed discussion can be found in e.g. [@bob], Section 3.4).


So if our testing revolves around $H_0$, where does the alternative $H_1$ come into play? We can see the presence of $H_1$ in commonly used orderings, such as the *likelihood ratio* of the two hypothesis: $t(x, \mu) = p(x | H_0)/p(x|H_1)$. Furthermore, we can define a measure of *error* in relation to $H_1$: given $H_1$ is true, what's the probability of accepting $H_0$, even when the data come from $H_1$? This quantity is known as $\beta$ -- a *powerful* test would have low $\beta$, i.e. often rejects $H_0$ in favour of $H_1$ when it is true, leading to the quantity $1-\beta$ being known as the **power** of a test.

[Visualization of a hypothesis test](visualtest.svg){#fig-visualtest}

When framing things in terms of error, we can see that the confidence level $\alpha$ also represents an error: *the probability of rejecting $H_0$ given that it's true.* Why? If the observed data was truly generated from $H_0$, it will fall in the extreme proportion of the data exactly $\alpha$ of the time, since that's the way we designated the proportions to begin with. This would lead to the erroneous statement of rejecting $H_0$ (though we know this stops short of a *decision* on $H_1$).  We can see a representation of both $\alpha$ and $\beta$ in [@fig-visualtest].

The quantities $\alpha$ and $\beta$ are often called *type-I* and *type-II* error respectively. I forget this all the time (in particular which type means what) so rarely use these names, but include them for completeness.


#### $p$-values {-}

So we set up a test by choosing the size $\alpha$, obtain the distribution for our chosen test statistic $t(x)$, and calculate the cutoff value $t_c$. What next? We'll take our observed data $x_0$, calculate $t(x_0)$, and see if it lies past $t_c$. If so, we'll reject $H_0$. If so, we'd maybe like to know by how much the data was incompatible with $H_0$. This quantity is known as the $p$-value, and is just the area under $p(t(x) | H_0)$, i.e.

$$ p_{x_0} = \int_{x_0}^{+\infty}p(t(x) | H_0) dx ~.$$

We can then rephrase a hypothesis test as rejecting $H_0$ when $p_{x_0} < \alpha$, and accepting $H_0$ otherwise. A low $p$-value is then a sign of low compatibility between the observed data and $H_0$. (Here, my notation has come back to bite me... $p_{x_0}$ is absolutely *not* the probability of $x_0$!!!)

#### Optimality {-}

What defines an optimal test? Well, $\alpha$ is specified prior to the test, so we're left with maximizing the *power* $1-\beta$. The task then becomes: given $\alpha$, choose a test statistic $t(x)$ such that the power $1-\beta$ is maximized.

In the case where $H_0$ and $H_1$ are simple, the likelihood ratio is the optimal test statistic in the sense of power, which can be proved through the Neyman-Pearson Lemma (omitted here for the sake of brevity, though see a visual proof in [@kyleproof]).

When we have a composite hypothesis, things get trickier, since we've only worked out the best test statistic for two point hypotheses. Instead of most powerful test, we need to change our language: we want the *uniformly most powerful test* across the many point hypotheses contained within the composite hypothesis.

A proxy way to address this is the following: we know that the likelihood ratio is optimal for two point hypothesis. If we're testing a point null ($\mu=\mu_0$) against a composite alternative ($\mu \neq \mu_0$), we can represent the alternative through the *best-fit hypothesis to the observed data*, i.e. estimate $\hat{\mu}$ via maximum likelihood. We can then use the test statistic $p(x | \mu_0) / p(x | \hat{\mu})$. This is exactly analogous to the use of a likelihood ratio ordering in confidence intervals!

### Duality between hypothesis test and intervals

It was probably hard to watch the number of times I defined "extreme" in this very specific way that seems to apply to both intervals and tests, without commenting on how they're similar or different. That's because they're one-to-one in many ways!

Say we make a $\mathrm{C.L.} = 95%$ confidence interval of $[\mu_1, \mu_2]$ given an ordering principle (e.g. likelihood ratio), a pdf $p(x | \mu)$, and some observed data $x_0$. This interval contains all values of $\mu$ for which the observed data $x_0$ is deemed *not* extreme. Given the *same ordering principle*, a hypothesis test of some particular $\mu_0$ asks: does $x_0$ lie in the extreme fraction of data $\alpha = 1-\mathrm{C.L.}$ when $\mu=\mu_0$? This is the *same thing* as asking: does $\mu_0$ lie in the confidence interval $[\mu_1, \mu_2]$? If so, then the acceptance interval for $p(x | \mu_0)$ will be intercepted by $x_0$ -- this would *accept* $H_0$ in the testing framework, since $x_0$ under assumption of $\mu_0$ is not considered extreme.

I hope this is clear enough, it took me a little while to see initially. Also, I've been bit clumsy with my language when talking about orderings and test statistics -- they are equivalent here, but it







### Bayesian procedures

#### Posterior estimation

#### Credible intervals
