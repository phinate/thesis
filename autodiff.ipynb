{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic differentiation\n",
    "\n",
    "$F: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ is a scalar valued function $F$ that takes in a vector input of dimensionality $n$, and returns a scalar.\n",
    "\n",
    "Can also write as $F(\\mathbf{x}\\in\\mathbb{R}^n) \\rightarrow y \\in \\mathbb{R}$.\n",
    "\n",
    "If we could break down $F$ into a composition of e.g. four other functions, we would write that as \n",
    "$F = D \\circ C \\circ B \\circ A$, which is equivalent to $y = F(\\mathbf{x}) = D(C(B(A(\\mathbf{x}))))$, where we can interpret the computation as starting from the inner level, i.e. application of $A$ to $\\mathbf{x}$, then $B$ to $\\mathbf{a} =$ the output of $A(\\mathbf{x})$), and so on. Let's also define $\\mathbf{b} = B(\\mathbf{a})$, $\\mathbf{c} = C(\\mathbf{b})$, and $y = D(\\mathbf{c})$.\n",
    "\n",
    "Function composition is important to cover, since a core idea of autodiff is to break down the gradient of a whole into the composition of the gradient of its parts via the chain rule.\n",
    "\n",
    "Recall the **Jacobian matrix**:\n",
    "\n",
    "$$\n",
    "\\mathbf{J}(F) = F'(\\mathbf{x}) =  \\left[ \\frac{\\partial y}{\\partial x_1} , \\frac{\\partial y}{\\partial x_2}, \\cdots, \\frac{\\partial y}{\\partial x_n}\\right]\n",
    "$$\n",
    "\n",
    "For our example, we can break this down into a product of individual Jacobian matricies for each of the intermediate functions:\n",
    "\n",
    "\n",
    "\n",
    "One can see the sizes of each of the intermediate matricies:\n",
    "\n",
    "- `size($D'(\\mathbf{c})$) = (1,len(c))`\n",
    "- `size($C'(\\mathbf{b})$) = (len(c), len(b))`\n",
    "- `size($B'(\\mathbf{a})$) = (len(b), len(a))`\n",
    "- `size($A'(\\mathbf{x})$) = (len(a), n)` (horizontal size on the last image is n)\n",
    "\n",
    "The size of the final Jacobian matrix is then  `(1, n)`, as shown earlier.\n",
    "\n",
    "This kind of sequential matrix multiplication can be called \"accumulating\" the Jacobian piece-by-piece. The order of the multiplication of these matricies (i.e. where to put the parentheses) can matter to optimise computational load, but two cases are of particular interest: \n",
    "\n",
    "- **Forward accumulation**: ****Start from the input and work forwards to the output (right to left)\n",
    "\n",
    "$$\n",
    "F'(\\mathbf{x}) = \\frac{\\partial y}{\\partial \\mathbf{c}}\\left(\\frac{\\partial \\mathbf{c}}{\\partial \\mathbf{b}}\\left(\\frac{\\partial \\mathbf{b}}{\\partial \\mathbf{a}} \\cdot \\frac{\\partial \\mathbf{a}}{\\partial \\mathbf{x}}\\right)\\right),\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{b}}{\\partial \\mathbf{a}} \\cdot \\frac{\\partial \\mathbf{a}}{\\partial \\mathbf{x}}\\ =\\frac{\\partial \\mathbf{b}}{\\partial \\mathbf{x}}=\\left[\\begin{array}{ccc}\\frac{\\partial b_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial b_{1}}{\\partial x_{n}} \\\\\\vdots & \\ddots & \\vdots \\\\\\frac{\\partial b_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial b_{m}}{\\partial x_{n}}\\end{array}\\right].\n",
    "$$\n",
    "\n",
    "- **Reverse accumulation**: vice-versa!\n",
    "\n",
    "$$\n",
    "F'(\\mathbf{x}) = \\left( \\left( \\frac{\\partial y}{\\partial \\mathbf{c}} \\cdot \\frac{\\partial \\mathbf{c}}{\\partial \\mathbf{b}} \\right)\\frac{\\partial \\mathbf{b}}{\\partial \\mathbf{a}} \\right)\\frac{\\partial \\mathbf{a}}{\\partial \\mathbf{x}},\n",
    "$$\n",
    "\n",
    " \n",
    "\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial \\mathbf{c}} \\cdot \\frac{\\partial \\mathbf{c}}{\\partial \\mathbf{b}}\\ =\\frac{\\partial y}{\\partial \\mathbf{b}}= \\left[ \\frac{\\partial y}{\\partial b_1} , \\frac{\\partial y}{\\partial b_2}, \\cdots, \\frac{\\partial y}{\\partial b_n}\\right]\n",
    "$$\n",
    "\n",
    "Note that the size of every intermediate matrix product is always `(1, -)`, meaning if we have this situation where we're going from $\\mathbb{R}^n \\rightarrow \\mathbb{R}$, reverse accumulation is much more efficient in terms of memory usage and compute, since we're only ever considering vectors and not matricies. This is the typical setting with a neural network: we have a very large input space (parameters), and we want to evaluate the Jacobian matrix of a scalar (loss function) with respect to those parameters.\n",
    "\n",
    "If we had the complementary setting, i.e. $\\mathbb{R} \\rightarrow \\mathbb{R}^n$, we would probably want to compute the Jacobian with forward accumulation instead, and apply the logic above in reverse.\n",
    "\n",
    "### Jacobian-vector/vector-Jacobian products (JVP/VJPs)\n",
    "\n",
    "Say we wanted to compute the product between the Jacobian of $F$ and some vector $v$, which we'll abbreviate to JVP (Jacobian-vector product). Despite the purpose of this query being not obvious at first — let alone warranting it a dedicated nickname —  it will prove to be useful in just a few lines, so hang tight.\n",
    "\n",
    "We can write this query with the same operation ordering as with the *forward* accumulation of a Jacobian:\n",
    "\n",
    "$$\n",
    "F'(\\mathbf{x})\\,\\mathbf{v} = \\frac{\\partial y}{\\partial \\mathbf{c}}\\left(\\frac{\\partial \\mathbf{c}}{\\partial \\mathbf{b}}\\left(\\frac{\\partial \\mathbf{b}}{\\partial \\mathbf{a}} \\left(\\frac{\\partial \\mathbf{a}}{\\partial \\mathbf{x}} \\cdot \\mathbf{v} \\right) \\right)\\right)\n",
    "$$\n",
    "\n",
    "Thinking of the rules of matrix multiplication, we note that $\\frac{\\partial \\mathbf{a}}{\\partial \\mathbf{x}} \\cdot \\mathbf{v}$ is only tractable if $\\mathbf{v}$ is of size `(n, 1)`, since $\\frac{\\partial \\mathbf{a}}{\\partial \\mathbf{x}}$  is of size `(len(a), n)`. Provided this is the case, all following computations will include 1 as one of the outer dimensions, meaning we once again only need to consider intermediary vectors instead of matricies when computing a\n",
    "\n",
    "What does it mean to \"differentiate through\" something?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "651f2bad27d44a91877bf2ed58cd15f6a905a1a50516233d74eca5c893c3c6fd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
