---
title: "Probability and Statistics, in practice"
format:
  html:
    code-fold: true
    default-image-extension: svg
  pdf:
    default-image-extension: pdf
jupyter: python3
---


This section focuses on how fundamental statistical ideas are translated into meaningful physics insights. We'll look at common practice, and summarize the main components needed to set the scene for applications that involve building new ideas with these techniques.

## Estimating distributions from data

Often we're not equipped with a way to describe some data we're interested in using a probability distribution. In that situation, it's useful to have a set of **density estimation** techniques within your toolkit. Here we go over a couple.

### Histograms {#sec-hists}

Ah yes, the infamous histogram. Exceedingly simple by design, it approximates a data distribution through counting the number of data points that lie in a set of adjacent disjoint intervals, or **bins**. A histogram, then, is expressible as a set of counts and a set of bin edges. See some example histograms in @fig-hist to see how the binning can affect the overall envelope of the distribution.

```{python}
#| label: fig-hist
#| fig-cap: "Histogram of some bi-modal data $x$, shown with different binnings."
import numpy as np
import matplotlib.pyplot as plt

subplot_settings = dict(figsize=[7, 3], dpi=200, tight_layout=True)
num_points = 1000
data = np.concatenate((np.random.normal(size=num_points), np.random.normal(size=num_points//2)+3))
fig, axs = plt.subplots(1,3, **subplot_settings, sharex=True, sharey=True)
ax=axs[0]
ax.hist(data, bins=10)
ax.set_ylabel("frequency")
ax.set_xlabel("x")
ax.set_title("10 uniform bins")
ax=axs[1]
ax.hist(data, bins=100)
ax.set_xlabel("x")
ax.set_title("100 uniform bins")
ax=axs[2]
ax.hist(data, bins=[-2,-1,-.5,0,1,4,6])
ax.set_xlabel("x")
ax.set_title("edges = [-2,-1,-0.5,0,1,4,6]");
```

The area under the histogram is equal to $\sum_{\mathrm{bins~i}} \mathrm{count}_i \times \mathrm{bin~width}_i$; we can force this to unit area by dividing each term in the sum by the bin width and the total number of counts. This produces something that can be interpreted as a (discrete) probability density, which can be useful when looking at just the shape of the distribution, for instance.

#### Why histograms in HEP? {-} {#sec-whyhist}

I [asked this question on Twitter](https://twitter.com/phi_nate/status/1251124042012274693?s=20&t=_miEUw-BGQwuWl1rsgyHuQ) because I was confused: the HEP analysis paradigm has the histogram as a central object, but why? The reasons I discovered are as follows:

- **Data structures**: the histogram has many benefits as a vessel to store data, e.g. their memory footprint is *independent of the size of the input data* -- large numbers for the counts are still just single numbers! They also have effectively no cost to evaluate (you just look up the count number based on the bin)
- **Poisson modelling**: a simple and tractable way to model the likelihood of a collider physics process is with a Poisson-based likelihood function, which has an expected number of counts that is parametrized using templates from signal and background processes. When you make a histogram of your physics quantities, you can model it in this way through having one Poisson distribution per bin!

There was also more in that thread on ease parallel computation, the fact that histograms are good at respecting physical boundaries, and some birds-eye view perspectives on how things are (and could be) done in the field. Many thanks to Kyle Cranmer, Jim Pivarski, Stan Seibert, Nick Smith, and Pablo De Castro for contributing to that discussion -- I encourage you to check out the thread!

### Kernel density estimation

If you wanted a smooth distribution instead of a discrete one, the *kernel density estimate* (KDE) has you covered.

It's a pretty simple but powerful idea: for each data point, define some *kernel function* that uses the point as a centre (e.g. normal distribution). Then, the distribution of the data at a point $x$ is equal to the *average of the kernel functions evaluated at $x$.

There are many different choices of kernel function, each with their own tradeoffs, but the most common one in practice is indeed the standard normal distribution $\mathrm{Normal}(0, 1)$. If we specify the mean as the data, then there's one missing ingredient -- the *width* of these distributions. That number is called the **bandwidth**, and controls the width of every kernel at once. Interestingly, the choice of bandwidth affects the resulting shape in general much more than the choice of kernel -- see @fig-kde for some examples of the bandwidth's influence on the distribution.

```{python}
from scipy.stats import gaussian_kde

grid = np.linspace(-3,6, 500)

fig, axs = plt.subplots(1, 5, **subplot_settings, sharex=True, sharey=True)
ax = axs[0]
kde = gaussian_kde(data, bw_method=0.01)
ax.plot(grid, kde.pdf(grid))
ax.set_ylabel("density")
ax.set_xlabel("x")
ax.set_title("bandwidth = 0.01", fontsize="medium")
ax = axs[1]
kde = gaussian_kde(data, bw_method=0.05)
ax.plot(grid, kde.pdf(grid))
ax.set_xlabel("x")
ax.set_title("bandwidth = 0.05", fontsize="medium")
ax = axs[2]
kde = gaussian_kde(data, bw_method=0.1)
ax.plot(grid, kde.pdf(grid))
ax.set_xlabel("x")
ax.set_title("bandwidth = 0.1", fontsize="medium")
ax = axs[3]
kde = gaussian_kde(data, bw_method=0.5)
ax.plot(grid, kde.pdf(grid))
ax.set_xlabel("x")
ax.set_title("bandwidth = 0.5", fontsize="medium")
ax = axs[4]
kde = gaussian_kde(data, bw_method=1)
ax.plot(grid, kde.pdf(grid))
ax.set_xlabel("x")
ax.set_title("bandwidth = 1", fontsize="medium");
```

Some talk on a midpoint between KDEs and histograms will appear in the applications part of the thesis!

### Fitting an existing distribution

If you have a decent idea on a distribution that may reasonably describe your data, you can simply perform a maximum-likelihood optimization to fit the parameters of the model to the data. One can even compose multiple distributions into a more complex likelihood. Not too much more to say about this, as it essentially comes under point estimation of a model parameter, which we talked about in the previous chapter!

### Other data-driven methods

We'll talk more about these in the machine learning section, e.g. Gaussian processes and normalizing flows. These are generally reserved for when you need a little bit of extra work in order to get a robust result, or to go beyond 1-D and 2-D variables in a scalable way.

## HistFactory: modelling nature as a set of counts

HistFactory [@hifa] is by far the most common statistical modelling tool used for collider physics data analysis. It's known for being difficult to understand at first -- if you've ever seen the full expression for the general likelihood, you'll have wondered if there was a need to extend the Greek alphabet to write down all the symbols that are used. Here, we'll take a slower and more gentle approach, building up the HistFactory likelihood piece-by-piece, until hopefully it's clear enough what's going on.

### Baseline model for a chosen statistic

Given some (new) physics process exists, we may expect $\lambda$ events to appear in our detector from that process. This number could come from e.g. simulating the physics process. It could also come from e.g. some data driven extrapolation method, but I'm going to call all of this simulation for the sake of the arguments below. The point is that we estimate it before looking at the important data in the region that would contain new physics.

So: say we run our detector, and we record $n$ *independent* events. What's the likelihood of observing these $n$ events with $\lambda$ expected from simulation?

We know from the previous chapter that this is modelled well with a Poisson distribution:

$$
p(n|\lambda) = \mathrm{Poisson}(n|\lambda)~.
$$ {#eq-poisson}

Along with the overall number of events we recorded, we may pick some statistic of the data $x$ that we choose to measure. That variable will have a distribution we can predict from simulation. How do we describe it?

We know that our data is divided into two categories: stuff that came from physics we're interested in (**signal**), and stuff that came from everything else (**background**). We can then say we have $s$ signal events in our sample and $b$ background events, with $s+b=\lambda$, our overall number of expected counts from simulation.

Each value of $x$ that comes from a signal event can be viewed as a sample from the unknown signal distribution $f_s(x)$, and likewise for background $f_b(x)$. We can even think of any particular value we measured (e.g. $x_0$) as being "marked" with an extra number -- either $f_s(x_0)$ if it came from signal, or $f_b(x_0)$ if it belongs to the background. This means that our overall distribution for the variable $x$ is described by "$s$" much of $f_s(x)$, and "$b$"-much of $f_b(x)$, i.e. for any value of $x$, it's density is then

$$
p(x) = \frac{sf_s(x) + bf_b(x)}{s+b}~,
$$ {#eq-balance}

where we choose to normalize by the total number of events $s+b$ to treat $p(x)$ as a proper density.

We can then model the whole dataset $\{x_i\}_{i=1}^n$ by multiplying the densities of all the individual events, since we assumed they were independent (otherwise we couldn't use the Poisson distribution!). We can then incorporate @eq-poisson with @eq-balance through multiplication of the densities:

$$
p(\{x_i\}_{i=1}^n) = \mathrm{Poisson}(n|s+b)\prod_{i=1}^n\frac{sf_s(x_i) + bf_b(x_i)}{s+b}~.
$$ {#eq-histcomb}

Notice that we don't have any free parameters right now -- the counts $s$ and $b$ will be fixed from our physics simulation once we get around to it. But what if wanted to infer the amount of signal present in our data? How would we do that? We can accomplish this through a little trick: we can multiply the number of signal events $s$ with an additional number $\mu$ that controls the overall **signal strength**. Estimating the value of $\mu$ would then tell us information about the amount of signal present in the data (assuming our model is accurate enough)!

We can now replace $s$ with $\mu s$ in @eq-histcomb to get

$$
p(\{x_i\}_{i=1}^n | \mu) = \mathrm{Poisson}(n|\mu s+b)\prod_{i=1}^n\frac{\mu sf_s(x_i) + bf_b(x_i)}{\mu s+b}~.
$$ {#eq-bothparts}

In this formalism so far, we've kept things generalized to the "unknown" pdfs $f_s(x)$ and $f_b(x)$, but we don't actually have access to them. We can approximate them using a KDE or some other method, but it's more common to find us with a *histogram* for this representation (reasons for why are outlined in @sec-whyhist).

Say we histogram our simulated signal and background data with the same binning (not required to be uniform) that uses a number of bins $k$, giving us sets of counts $\mathbf{h}^{\mathrm{sig}} = \{h_1^{\mathrm{sig}}, h_2^{\mathrm{sig}}, \dots, h_k^{\mathrm{sig}}\}$ and $\mathbf{h}^{\mathrm{bkg}} = \{h_1^{\mathrm{bkg}}, h_2^{\mathrm{bkg}}, \dots, h_k^{\mathrm{bkg}}\}$. Recall from @sec-hists that we can use these to approximate a density by normalizing with respect to bin width and number of events. Then, if we say a given value of $x$ falls into some bin with index $j$, we can write that the density at $x$ is approximated by the (normalized) count in that bin, for both signal and background separately:

$$
f_s(x \in \mathrm{bin~}j) \approx \frac{h^{\mathrm{sig}}_j}{s\times\mathrm{width~of~bin}~j};~~~f_b(x \in \mathrm{bin~}j ) \approx \frac{h^{\mathrm{bkg}}_j}{b\times\mathrm{width~of~bin}~j}
$$


<!-- Fill me in with possible details on going to a binned model -->

From here, we can express @eq-bothparts as a product over bins $j$ instead of events $i$:

$$
p(\{n_j\}_{j=1}^k | \mu) = \mathrm{Poisson}(n|\mu s+b)\prod_{j=1}^k\frac{\mu h^{\mathrm{sig}}_j  + h^{\mathrm{bkg}}_j}{\mu s+b}~.
$$

Note that we've shifted from talking about values of $x_i$ to bin counts $n_j$, where $\sum_{j=1}^k n_i = n$. These counts don't seem to appear in the likelihood yet, but we can make this explicit through the following relation^[This relation arises from noticing that $\lambda^n = \lambda^{\sum n_j} = \prod_j \lambda^{n_j}$, and using it to manipulate the Poisson on the left-hand side, amongst other things. We gloss over the full working, but I would like to include it if I have time (reading this means I probably didn't, but I do plan to release a blog post in future).]:

$$
\mathrm{Poisson}(n|\mu s+b)\prod_{j=1}^k\frac{\mu h^{\mathrm{sig}}_i  + h^{\mathrm{bkg}}_i}{\mu s+b} \propto \prod_{j=1}^k \mathrm{Poisson}(n_j | \mu h^{\mathrm{sig}}_j  + h^{\mathrm{bkg}}_j)~,
$$

where the constant of proportionality is a factor involving factorials of the individual counts (also referred to as combinatorics). Since we don't care about the overall normalization when we do inference (e.g. the maximum likelihood value is independent of the scale, and the normalization cancels in a likelihood ratio), we will consider this proportionality as an equivalence.

These gymnastics have left us with the following likelihood:

$$
p(\{n_j\}_{j=1}^k | \mu) = \prod_{j=1}^k \mathrm{Poisson}(n_j | \mu h^{\mathrm{sig}}_j  + h^{\mathrm{bkg}}_j)~,
$$ {#eq-hifabase}

which is simply a product over Poisson distribution for each bin within a histogram, where we expect a contribution of $\mu h^{\mathrm{sig}}_j  + h^{\mathrm{bkg}}_j$ from each of signal and background respectively per bin $j$. This expression forms the core of the HistFactory approach.

### Uncertainty modelling through nuisance parameters


 Now, we'll extend the model from @eq-hifabase in a similar way to when we added $\mu$ to manipulate the signal scale, but this time, it's in order to be able to model uncertainties. There are many different types of uncertainties, so we'll cover them all in-turn.

 #### Systematic uncertainties {-}

 The origin of systematic uncertainties in simulation is this: we're uncertain as to the true values of the physics parameters that we should put into the simulator. Let's denote an example parameter with $\alpha$. To quantify how varying $\alpha$ changes the likelihood in the ideal world, we would just include those parameters of the simulator within our likelihood model. However, this would require the ability to simulate data on-the-fly at any given value of the physics parameter, and then propagate that change all the way through our analysis selection requirements. This is difficult from both a practical and a computational perspective (the simulators we use are expensive to evaluate), plus it would have to be done for all the parameters we may want to model in this way. So what do we do instead?

We may have a best prediction for $\alpha$ from studies carried out by e.g. a performance group in your collaboration that focuses on measuring $\alpha$, but we'll also have some notion of uncertainty on that value, perhaps in the form of a distribution on $\alpha$. An example procedure that we often do in this case is to just simulate our physics data at the best guess for that parameter $\alpha$ -- we'll refer to this as the **nominal** value $\alpha_{\mathrm{nom}}$ -- and then also simulate data for values at $\alpha_{\mathrm{nom}}+\sigma_{\alpha} = \alpha_{\mathrm{up}}$ and $\alpha_{\mathrm{nom}}-\sigma_{\alpha} = \alpha_{\mathrm{down}}$, where $\sigma_{\alpha}$ is some notion of a standard deviation on $\alpha$ (e.g. calculated arithmetically on a  sample or fitted as part of a normal distribution).

We're not restricted to the choice of $\alpha_{\mathrm{up}}$ and $\alpha_{\mathrm{down}}$, but it's pretty commonplace as a quick way to get a rough idea of the influence of $\alpha$ on the histograms. And that's the point -- we really only care about how varying $\alpha$ changes the resulting histogram for that process -- either $\mathbf{h}^{\mathrm{sig}}$ or $\mathbf{h}^{\mathrm{bkg}}$. We can then estimate the effect of $\alpha$ as a continuous change by some kind of *interpolation between the resulting histogram yields*. More on the methods of interpolation later.










#### Interpolation {-} {#sec-interp}




## Hypothesis testing and asymptotic formulae in HEP

In @sec-hyptests, which covered frequentist hypothesis tests, we noted that we don't necessarily have access to the *sampling distribution* of the test statistic $p(t(x)|H)$ given a particular hypothesis $H$. One way to estimate $p(t(x)|H)$ is to simply calculate $t(x)$ for many samples $x \sim p(x|H)$, and build up the distribution empirically. However, there exist some choices of $t(x)$ that give us *asymptotic guarantees* as to the form of $p(t(x)|H)$, i.e. we can fairly reliably know its shape as long as we have a decent enough sample size of $x$.

One of these choices that we've seen a couple times already is the likelihood ratio between a point null at $\mu$ and a composite alternative represented by the maximum likelihood point $\hat{\mu}$:

$$
R(x, \mu) = \frac{p(x|\mu)}{p(x|\hat{\mu})}~.
$$

We'll likely have to deal with nuisance parameters in the likelihood, for which we extend this as shown in @eq-profile-lhood-ratio to:

$$
\lambda(x, \mu) = \frac{p\left(x|\mu,\hat{\hat{\theta}}(\mu)\right)}{p\left(x| \hat{\mu}, \hat{\theta}\right)},
$$

where we recall that $\hat{\hat{\theta}}(\mu)$ represents fitting the value of $\theta$ while holding $\mu$ fixed at it's value from the input to $\lambda$.

This quantity (or $-2\ln$ of it at least) forms the basis of all test statistics that we use in tests for the discovery of a new particle, or for setting a limit on a physical quantity (e.g. a particle mass or process cross-section).

### Sampling distributions for -2 \ln \lambda

The first result we'll exploit to our advantage is that of Wald @wald, who showed that we can relate $\lambda(x,\mu)$ with the maximum likelihood estimate $\hat{\mu}$ in the following way:

$$
-2\ln \lambda(x,\mu) = \left(\frac{\mu - \hat{\mu}(x)}{\sigma_{\hat{\mu}}}\right)^2 + \mathcal{O}(\frac{1}{\sqrt{N}})~,
$$ {#eq-wald}

where $\sigma_{\hat{\mu}}$ is the standard deviation $\hat{\mu}(x)$, which will tend to a normal distribution with sufficient data, and $N$ is our data size. This equation is known as **Wald's relation**. Note that this is just a quadratic in $\hat{\mu}(x)$  -- the plot of $-2\ln \lambda(x,\mu)$ vs $\hat{\mu}(x)$ will be a parabola (to the extent that we can neglect the $\mathcal{O}(1/ \sqrt{N}$ term).

Another interesting result is that since we're already incorporating nuisance parameters within $\lambda$, the shape of the test statistic against $\hat{\mu}(x)$ will follow this relation independent of the value of the nuisance parameters.

How do we go from here to $p(-2\ln \lambda(x,\mu) | H)$? Well, $H$ can be represented as a value of $\mu$ which we'll call $\mu'$ -- it either




Given that we're interested in probability models of the form in @eq-hifabase, we can derive some
