{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Physics Analysis as a Differentiable Program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "Given the success of the Standard Model (SM), analysis of data from the LHC usually occurs for two reasons:\n",
    "- Precisely measuring SM processes to look for small deviations with SM predictions\n",
    "- Searching for new physics signatures as predicted by models beyond the SM\n",
    "\n",
    "When analysing data in this way, we'll have lots of free parameters to tune. These can be as simple as a threshold value that you limit the p_T to, or as complicated as the weights and biases that determine a neural network for identifying $b$-jets. We can of course choose any values for these quantities to do our analysis, but the resulting physics that follows may suffer as a result. As such, we're likely to try some kind of optimization to improve the answers to our physics questions. How do we do this in practice?\n",
    "\n",
    "In either case above, there is a notion of <span style=\"color:#13becf\">signal</span> (what youâ€™re looking for) and <span style=\"color:#ff7f0e\">background</span> (everything else).\n",
    "Generally, we then try to choose a parameter configuration that can separate (or discriminate) the signal from the background, allowing us to extract just the data we think is relevant to the physics process we're looking at. As an example, machine learning models are often trained using the **binary cross-entropy** loss as an objective, which corresponds to optimizing the ability of the model to identify whether an event originated from signal or background processes. A closely related goal is the **Asimov significance** in the case of signal and background event counts $s$ and $b$ with *no uncertainty* on either quantity. The formula for this stems from assuming a Poisson likelihood function, and is equal to\n",
    "\n",
    "$$Z_A = \\sqrt{2\\sum_{i\\in bins}((s_i + b_i)(\\log{(1 + s_i / b_i)}) - s_i)}.$$\n",
    "\n",
    "As indicated in the sum, these counts can be spread across different bins in the case where your data is a histogram, but the formula is more commonly reduced to the 1-bin scenario that just deals with the overall numbers of signal and background events. In this case, we can then Taylor expand the logarithm to get\n",
    "\n",
    "$$Z_A = \\sqrt{2((s+b)(s/b + \\mathcal{O}(s/b)) - s} \\approx s/\\sqrt{b}~~~\\mathrm{for}~s<<b.$$\n",
    "\n",
    "This makes it much clearer to see that optimising with respect to $Z_A$ is just a fancier way of trying to increase the amount of signal compared to the amount of background, which is directly analogous to separating signal from background, just as binary cross-entropy would do.\n",
    "\n",
    "Now, this is all very sensible of course (we want to discover our signal), but this approach has some shortcomings that distance the efficacy of the resulting configuration from our physics goals. A recent review of deep learning in LHC physics [@deeplhc] lets us in on why:\n",
    "\n",
    "> (...) tools are often optimized for performance on a particular task that is **several steps removed from the ultimate physical goal** of searching for a new particle or testing a new physical theory.\n",
    "\n",
    "> (...) sensitivity to high-level physics questions **must account for systematic uncertainties**, which involve a nonlinear trade-off between the typical machine learning performance metrics and the systematic uncertainty estimates.\n",
    "\n",
    "This is the crux of the issue: we're not accounting for uncertainty. Our data analysis process comes with many sources of systematic error, which we endeavour to model in the likelihood function as nuisance parameters.\n",
    "- e.g. simulation comes with many physics parameters that we have to choose, e.g. jes\n",
    "- systs between simulators\n",
    "- quantification of data/MC agreement (closure?)\n",
    "...etc\n",
    "\n",
    "But this is all talk... let's prove it!\n",
    "\n",
    "[anal example]\n",
    "\n",
    "This motivates a search for an objective function that can capture these sources of uncertainty.\n",
    "\n",
    "Attempts:\n",
    "- Asimov sig with assumptions on bkg uncert: [@asimovuncert]\n",
    "- Learning to pivot: [@pivot]\n",
    "- Directly incorporate NPs: [@uncert]\n",
    "\n",
    "But, don't we already have the goal we're looking for?\n",
    "\n",
    "We assess physics results by their **significance**, which is a quantity that's one-to-one with a $p$-value coming from a hypothesis test of just background events v.s. additional events from the existence of the signal process. This implies a likelihood model for both of these cases, typically covered in collider physics by the HistFactory setup for building likelihoods based on event counts. In these likelihoods, we're also careful to include all the details of the systematic uncertainties that we're able to quantify by constructing nuisance parameters that vary the shape and normalization of the model. From here, to calculate the $p$-value, we then construct the **profile likelihood ratio** as a test statistic, which accounts for these systematic uncertainties by fitting the value of the nuisance parameters depending on the hypothesis you test (link detail from section).\n",
    "\n",
    "This seems like a good candidate for an objective function! So why haven't we done this already?\n",
    "\n",
    "As made clear in (ref sec), if we want to perform optimization using gradient-based methods (footnote: don't have to use gradient based methods!), then we need the objective that we optimize to be *differentiable*. This is not immediately the case for the significance -- we would have to be able to differentiate through all stages of the full calculation, including model building, profiling, and even histograms, which are not generally known for their smoothness. But say we were able to decompose this complicated pipeline into bite-size chunks, each of which we can find a way to take gradients of. What becomes possible then? This begins our view of **physics analysis as a differentiable program**.\n",
    "\n",
    "In the following sections, we'll take a collider physics analysis apart step-by-step, then see how we can employ tricks and substitutes to recover gradients for each piece. After that, we'll explore the ways that we can use the result to perform gradient-based optimization of different parts of the analysis with respect to physics goals. We'll then do it all at once by*optimizing a toy physics analysis from end-to-end*, exploring the common example of a summary statistic based on a neural network, accounting for uncertainties all the while.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<!-- This implies that we need to specify null and alternative hypotheses, which are taken to be background and signal + background respectively in the case of discovery (when setting limits, we flip the roles of the null and alternative * why?) for which we assume the existence of statistical models for each of these processes. The vast majority of collider physics analyses handle this through the HistFactory likelihood (ref section), which at it's simplest, can be viewed as \n",
    "\n",
    "$$L(\\mu | \\mathbf{x}) = \\prod_{bins~i}\\mathrm{Poisson}(x_i | \\mu s_i + b_i) $$\n",
    "\n",
    "for fixed data $\\mathbf{x}$ and per-bin expected counts $s_i$ and $b_i$ for signal and background processes respectively. The parameter $\\mu$ then controls the relevant hypothesis: $\\mu$ = 0 corresponds to just the counts expected if we had only background events, and $\\mu$ = 1 includes the nominal prediction from the presence of our signal process on top of that. In the full HistFactory formalism, there are also additional terms and parameters that represent systematic uncertainties. -->\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making HEP Analysis Differentiable\n",
    "\n",
    "The goal of this section is to study components within a HEP analysis chain that are not typically differentiable, and show that when we overcome this, we can employ the use of gradient-based optimization methods. From there, we'll examine the typical steps needed to calculate the sensitivity of a physics analysis, and see how we can make that whole chain differentiable at once, opening up a way to incorporate the full inference procedure when finding the best analysis configuration.\n",
    "\n",
    "- something about surrogates vs gradient tricks (e.g. approximations or using theoretical results)\n",
    "\n",
    "- something about the fact that surrogates can be used *just* for optimization if the hard version of the op isn't desirable, e.g. assuming the histogram bins follow a poisson distribution (not true if you're approximating the underlying binning process)\n",
    "\n",
    "### Thresholding (cuts)\n",
    "- Cuts are step functions from logical less than or more than statements\n",
    "- Basically applies weights to the data -- 0 on one side of the threshold, and 1 on the other\n",
    "\n",
    "--plot\n",
    "\n",
    "- a differentiable version of this admits real numbers in-between these values, smoothing out the step\n",
    "- example: sigmoid\n",
    "- you can tune the steepness of the slope by adding a number \n",
    "--plot\n",
    "\n",
    "### Binned density estimation (histograms)\n",
    "\n",
    "Histograms are discontinuous by nature. They are defined for 1-D data as a set of two quantities: intervals (or *bins*) over the domain of that data, and counts of the number of data points that fall into each bin. For small changes in the underlying data distribution, bin counts will either remain static, or jump in integer intervals as data migrate between bins, both of which result in ill-defined gradients. To demonstrate this, let's examine what happens to a histogram when we shift the underlying distribution by a constant factor.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as sts\n",
    "from celluloid import Camera\n",
    "from IPython.display import HTML\n",
    "import jax.numpy as jnp\n",
    "import jax.scipy as jsc\n",
    "from jax.nn import softmax\n",
    "from relaxed import hist\n",
    "\n",
    "plt.rc(\"figure\", figsize=(10.0, 3.0), dpi=120, facecolor=\"w\")\n",
    "np.random.seed(222)\n",
    "\n",
    "\n",
    "def kde(x, data, h):\n",
    "    return jnp.mean(jsc.stats.norm.pdf(x.reshape(-1, 1), loc=data, scale=h), axis=1)\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "cam = Camera(fig)\n",
    "plt.xlim([-1, 4])\n",
    "plt.axis(\"off\")\n",
    "bins = np.linspace(-1, 4, 7)\n",
    "centers = bins[:-1] + np.diff(bins) / 2.0\n",
    "grid = np.linspace(-1, 4, 500)\n",
    "mu_range = np.linspace(1, 2, 100)\n",
    "data = np.random.normal(size=100)\n",
    "truths = sts.norm(loc=mu_range.reshape(-1, 1)).pdf(grid)\n",
    "for i, mu in enumerate(mu_range):\n",
    "    plt.plot(grid, truths[i], color=\"C1\")  # plot true data distribution\n",
    "    plt.hist(\n",
    "        data + mu, bins=bins, density=True, color=\"C0\", alpha=0.6\n",
    "    )  # histogram data\n",
    "    plt.axvline(mu, color=\"slategray\", linestyle=\":\", alpha=0.6)\n",
    "    cam.snap()\n",
    "animation = cam.animate()\n",
    "# HTML(animation.to_html5_video()) # uncomment for animation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "We address\n",
    "this inherent non-differentiability through implementing a\n",
    "differentiable surrogate: a histogram based on a *kernel density\n",
    "estimate* (KDE).\n",
    "\n",
    "A KDE is a \"non-parametric\\\" density estimate based on defining a kernel\n",
    "function $K$ centred on each data point. Then, the density at an\n",
    "evaluation point $x$ is the average of the contributions of each kernel\n",
    "function at that point.\n",
    "\n",
    "a with the full density given as $p(t) = 1/n\\sum_i K(t,t_i)$. Normally,\n",
    "a popular kernel function choice is the standard normal distribution,\n",
    "which comes with a parameter called the **bandwidth** that affects the\n",
    "smoothness of the resulting density estimate.\n",
    "\n",
    "Coming back to gradients: in our case, the data $t_i$ we construct the\n",
    "density estimate over are themselves functions of the summary statistic\n",
    "parameters, i.e. $t_i = f(x_i;\\phi)$. The resulting density estimate\n",
    "$p(t|\\phi)$ will then be differentiable as long as the kernel $K$ is\n",
    "differentiable with respect to $t_i$, and by extension with respect to\n",
    "$\\phi$. To extend this differentiability in a binned fashion, we can\n",
    "accumulate the probability mass of the KDE within the bin edges of the\n",
    "original histogram -- equivalent to evaluations of the Gaussian\n",
    "cumulative density function -- to convert $p(t|\\phi)$ to a **binned KDE\n",
    "(bKDE)**, i.e. a set of discrete per-bin probabilities $p_i(\\phi)$.\n",
    "\n",
    "In the limit of vanishing bandwidth, the bKDE recovers the standard\n",
    "histogram, but gradients become susceptible to high variance. Increasing\n",
    "the bandwidth can alleviate this, but at the cost of introducing a bias.\n",
    "There is then a trade-off between decreasing the bandwidth enough to\n",
    "minimise this bias, and increasing it enough to guarantee gradient\n",
    "stability[^2]. We can see a demonstration of this behaviour in\n",
    "[\\[fig:hist\\]](#fig:hist){reference-type=\"autoref\"\n",
    "reference=\"fig:hist\"}, where the bandwidth is tuned relative to the bin\n",
    "width.\n",
    "\n",
    "![Illustration of the bias/smoothness tradeoff when tuning the bandwidth\n",
    "of a bKDE, defined over 200 samples from a bi-modal Gaussian mixture.\n",
    "All distributions are normalised to unit area. The individual kernels\n",
    "that make up the KDE are scaled down for\n",
    "visibility.](relaxed_hist.pdf){#fig:hist width=\"\\\\textwidth\"}\n",
    "\n",
    "### Maximum likelihood estimation (fitting)\n",
    "\n",
    "### Frequentist statistical inference (significance)\n",
    "\n",
    "\n",
    "Given a pre-filtered dataset, a commonly used analysis pipeline in HEP involves the\n",
    "following stages:\n",
    "\n",
    "\n",
    "1.  Construction of a learnable 1-D summary statistic from data (with\n",
    "    parameters $\\phi$)\n",
    "\n",
    "2.  Binning of the summary statistic, e.g. through a histogram\n",
    "\n",
    "3.  Statistical model building, using the summary statistic as a\n",
    "    template\n",
    "\n",
    "4.  Calculation of a test statistic, used to perform a frequentist\n",
    "    hypothesis test of signal versus background\n",
    "\n",
    "5.  A $p$-value (or $\\mathrm{CL_s}$ value) resulting from that\n",
    "    hypothesis test, used to characterize the sensitivity of the\n",
    "    analysis\n",
    "\n",
    "We can express this workflow as a direct function of the input dataset\n",
    "$\\mathcal{D}$ and observable parameters $\\phi$:\n",
    "\n",
    "$$\n",
    "    \\begin{equation}\n",
    "    \\mathrm{CL_s} = f(\\mathcal{D},\\phi) = (f_{\\mathrm{sensitivity}} \\circ f_{\\mathrm{test\\,stat}} \\circ f_{\\mathrm{likelihood}}  \\circ f_{\\mathrm{histogram}}  \\circ f_{\\mathrm{observable}})(\\mathcal{D},\\phi).\n",
    "    \\end{equation}\n",
    "    $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
