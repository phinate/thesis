---
title: "Signal Model Interpolation using Normalizing Flows"
format:
  html:
    code-fold: true
    default-image-extension: svg
  pdf:
    default-image-extension: pdf
jupyter: python3
---

One of the primary goals of data analysis in collider physics is to test the viability of proposed models for new physics, which attempt to address the shortcomings of the well-tested Standard Model. We'll call these models the **signal**. Since we don't have access to real data from these hypothesized phenomena, we use high-quality simulations of the underlying physics to generate samples that mimic data collected from major experiments, e.g. the Large Hadron Collider. This then lets us identify the most prominent signature that physics process leaves in the data, such as the shape of the invariant mass of some particular final state. We can compare that signature with data generated to mimic the same final state, but with only standard model processes (which we term **background**), and use this comparison as a mechanism to produce an analysis that *could* discover the signal if it were present. This can then be applied to real data.

How many models are there to test? In general, there's no upper limit -- arXiv grows thicker every day with more theory model submissions. But what about within a certain class of model with *similar free parameters*? This scope-narrowing makes our lives much easier, as we're only now limited by the parameters of the model, e.g. the mass of a new particle. One could then imagine testing each possible value for this new mass... until we realize that there's only so many times we can do this. Recall that simulating data is in general expensive, and we'd rather avoid it where we can. Moreover, if we have a multi-dimensional parameter space to explore, covering that in an efficient way will be prohibitively expensive to simulate. How do we get around this?

Well, if we're interested in some signature from the model in a particular set of variables, we could generate that data for a set of parameters, then find some way to interpolate between them. Specifically, this can be viewed as trying to estimate the probability density (or *shape*) of a variable of interest, where we want to *condition* on the physics parameters involved in the generation of that shape. This is the approach taken here.

It's worth noting that physics analyses typically don't interpolate on the level of the statistical model, and instead just interpolate the result of the search itself, e.g. in the space of $p$-values or on limits for the signal strength $\mu$ across a grid of e.g. different signal masses. This is often quoted without error (a different issue), and does not take into account the fact that we'd expect to be less sensitive to new physics in the regions that we don't have high-quality simulations for. However, if one can interpolate the signal model in some way *before* calculating the final result, then incorporate a predictive uncertainty from that interpolation into the statistical treatment of the data, we should hopefully be able to have a more grounded representation of our sensitivity between simulated signal model points.

## Problem statement

Say a simulated signal process outputs a set $\mathbf{x}$ of per-event physics quantities (or **observables**), where each quantity is typically a real number (e.g. kinematics). We're interested in making a statistical model of this process, and so take interest in the distribution over $\mathbf{x}$, which we'll denote $p_s(\mathbf{x} | \theta)$, with $\theta$ representing the free parameters of the physical model (and $s$ standing for signal). We can partition $\theta$ as $\theta = [\phi, \nu]$, where $\phi$ effectively parametrizes the hypothesis of the new physics model (e.g. through the mass of a new particle), and $\nu$ represents parameters that are needed to specify the physics conditions, but are not of interest (e.g. a jet energy scale). We focus on conditioning with respect to $\phi$ here, and leave treatment of $\nu$ as part of the statistical modelling process (see @sec-hifa-nps for more on this).

We don't have a way to calculate this density due to our inability to access the implicit distribution defined by the simulator, but we do have access to the samples themselves (at some finite number of values of $\theta$). This means we can empirically estimate the distribution in some way, provided we have the right tools to do so. If successful, we're free to directly use this empirical estimation of $p_s(\mathbf{x} | \theta)$, but in practice, we're often faced with an additional modelling constraint: many analyses in collider physics operate using *histograms* of observables, and construct a statistical model based on that. As such, it would be desirable if we could find a density estimator that:

- allows for the *conditioning* of side variables (here, this would be conditioning on $\phi$)
- has a tractable likelihood (e.g. for direct use in "unbinned" analysis)
- can model discrete densities like a histogram
    - or, optionally, allow the generation of samples that can be histogrammed later
- (optionally) comes with a notion of uncertainty

This brings us to normalizing flows!

## Normalizing flows (recap)

A much more extensive motivation for normalizing flows can be found in @sec-flows, but we'll recap the main points here.

Normalizing flows are a class of density estimation technique, which attempts to transform a designated *base* density into a *target density with the aid of neural networks. This admits the ability to sample from a flow, since you can just transform the samples from the base density, which is usually of simple form (e.g. normal distribution). Moreover, flows have a tractable likelihood, which can be evaluated by transforming the samples using the learned transform, putting that into the base density, and multiplying by the relevant Jacobian, effectively bending the space into the desired shape to mimic the target density.

To train a flow, we simply fit samples from the base distribution to the training samples such that the negative log-likelihood of the flow model is minimized. This is done across batches of different training data, just as in standard machine learning optimization.

All of the properties of a flow satisfy most of the bullets above for our criteria, but we're still missing two parts: conditioning and uncertainty. These are addressed in the following sections.


### Implementing conditional density estimation

A few different types of flows allow for conditional density estimation, e.g.

### Estimating predictive uncertainty

If we're using the likelihood (or a histogram of the samples) to make a prediction using the flow, we


## Example 1: Following the mean

To test out the MAF as a conditional density estimator, it suffices to model some family of distributions parametrized by some context $\phi$, just as we set out in the problem statement. One thing I thought of when considering this is a 2-D normal distribution, where the mean represents the context. This should be a simple starting problem, and is a similar use case to that which I encountered in a physics context, where the signature left in the invariant mass spectrum clustered around the value of the rest mass of the particles that produced it. We'll add more complexity to this later.

We'll start by sampling a range of normal distributions with different means and unit covariance. The means here are the *context* -- the thing we'll condition on during training and inference. Training data will then consist of pairs of samples from these distributions, and the mean of the distribution that generated the samples.

We'll also split the different distributions up into context points that are used for training, and unseen points that are used for validation and testing (in practice, these should be two distinct tasks, but we'll combine them for these examples to portray the proof-of-concept in a best-case way). Points in the test set are not seen during training; we use them purely to assess performance (and select a good model) when interpolating to a context not seen before.

The distributions chosen are shown in @fig-normalgrid, where 49 different means have been evenly selected across a grid of points, indicated by crosses colored by the set they belong to. Note that the test set points are chosen such that they're all *inside* the grid -- this is because we're seeking to interpolate our results within the region we have data as opposed to extrapolate outside of it.

![A grid of 49 different normal distributions, each with a mean corresponding to a cross on the image, and with unit covariance. The contour representing 0 is omitted for visualization purposes.](images/flows/simple/simple_data_dists){#fig-normalgrid}

10,000 points are then drawn from each distribution, and 3 MAFs are trained on the same training data, but with different weight initializations. The hyperparameters for training those MAFs are as follows:

- Batch size of 4000 points
- 10 layers of linear, autoregressive transforms
    - Each transform's neural network has one hidden layer of 6 neurons
- Adam optimizer with a learning rate of 1e-3
- 3000 iterations total (common reasonable plateau point for all examples)

During training, the inputs and context are each scaled to standard normal distributions through fitting a `StandardScaler` from Scikit-Learn @scikit-learn to the train data. Any samples produced from the flow are then transformed back using this same scaler.

It's worth noting that since we trained three MAFs, when I refer to the "flow" in later sections, I'm referring to the ensemble of all three models, where their predictions (either likelihoods or histograms) are averaged together, and their predictive uncertainty is the corresponding standard deviations.

We can examine the results of the training through different types of visualizations, as we'll see below:

### Loss curves

See @fig-simple-loss for a comparison of the negative log-likelihood loss for train and test sets. This is for just one of the three flows, and shows the similarity of the train and test losses due to the fact that the train and test data are so similar (there's not much domain shift needed, just learning the position of the mean).

![Example loss curves from a single flow trained on the "following the mean" scenario. The model weights are extracted from the best test loss.](images/flows/simple/simple_example_loss){#fig-simple-loss}

### Distribution similarity

It's illustrative to plot the likelihoods learned by the flow for each of the context points, including the ones for which we didn't see during training. We could, in theory, supply any value of the context for this evaluation, but it's much more useful to do it in the region that we have test data so we can compare the result.

We can find this plot in @fig-simple-lhoods, where the plot from @fig-normalgrid is shown as a comparison metric side-by-side. We can immediately see that the likelihood shapes are about what we'd expect, but we note the contours are coloured in log scale for the flow, which is done so to enhance visibility of the shape close to the mean, since the contours increase more rapidly in value around the center of the distribution. Additionally, the values of the normalization are much higher for the flow, which can be attributed to the fact that the training data is very concentrated around the mean, and so a distribution with all it's density there will quickly give a good solution, as opposed to distributing that over a larger range in a more diffuse way. This could likely just be due to the training data being capped at 10k points.

![Left: the same grid of 49 different normal distributions from @fig-normalgrid. Right: the learned likelihood from the flow evaluated with context equal to the values of the means from the grid.](images/flows/simple/simpleflowvsdata){#fig-simple-lhoods}

This is a nice visual check, but it's probably more useful to quantify this similarity in some way.

### Histogram similarity

Since we're interested in possibly using the result as a histogram, we can directly compare histograms of samples from the flow with histograms of the original data. Plots of the raw data versus flow differences can be found in @fig-simple-hists-train for training points, and @fig-simple-hists-test for test points. The plots are made using a histogram of the whole dataset for each context point (10k data points in total per histogram) versus a histogram of 100k samples conditioned on the same context points from the flow to get the overall shape, which is then re-normalized back to 10k overall counts. The x-y axes are omitted for a cleaner plot, but the histogram range is around +/- 2.5 in each direction about the mean (imagine a square around one of the distributions in @fig-normalgrid).

![Differences between flow and data histograms across training context points, where each histogram is of 10k data points. The red cross marks the position of the mean of the true distribution.](images/flows/simple/simplehistdifftrain-reallyunscaled){#fig-simple-hists-train}

![Differences between flow and data histograms across unseen, test set context points, where each histogram is of 10k data points. The red cross marks the position of the mean of the true distribution.](images/flows/simple/simplehistdifftest-errors){#fig-simple-hists-test}

The main thing we can get from this visualization is the imprint that subtracting the flow histogram from the data leaves. We can see this more prominently in @fig-simple-hists-test, where there are some biases left from the flow network getting the area around the mean slightly off. Green represents where the data counts are larger, and brown represents where the flow predicts larger counts. The scale of the colorbar in each case is worth noting -- for a histogram of size 10k, these are relatively tame differences.

But these plots are only half the story; we'd much rather see how the flow *uncertainties* affect these results. Instead of doing this bin-by-bin where small localized uncertainties make for difficult visualization, we plot the *pulls* aggregated over all training and test points separately. The pull here for one bin is defined as

$$
\text{pull } = \frac{\text{flow prediction }-\text{ data prediction}}{\sigma_{\text{flow}}}~,
$$

where the uncertainty $\sigma_{\text{flow}}$ comes from the standard deviation of the counts from the three histograms in that bin, which were produced by sampling each flow in turn. Without making any strong theoretical claim, the reasoning to plot this quantity is to follow the intuition behind the central limit theorem, where we assume that the bin counts will tend to a normal distribution in the limit of many repeated experiments. If the flow is a good predictor of the mean of that normal distribution, we'd observe the data being distributed as approximately $\mathrm{Normal}(\text{flow}, \sigma_{\mathrm{flow}})$. This ballpark statement is somewhat realized in @fig-simple-pulls, where we plot histograms of the pull aggregated across all training contexts (left plot) and testing contexts (right plot). The results are fairly unbiased in either case, with a spread of pulls being fitted a little wider than a standard normal.

![Plot of the pull for training contexts (left plot) and testing contexts (right plot). A fitted normal distribution is also shown for illustrative purposes.](images/flows/simple/simpleflowpulls){#fig-simple-pulls}

## Example 2: Gaussian firework
