<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.251">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Data Analysis in High-Energy Physics as a Differentiable Program - 4&nbsp; Gradient descent</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./autodiff.html" rel="next">
<link href="./stat-practical.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Gradient descent</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Data Analysis in High-Energy Physics as a Differentiable Program</a>
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container">
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container">
  <a href="./index.html" class="sidebar-item-text sidebar-link">Preface</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container">
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">Fundamentals</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a>
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">
          <li class="sidebar-item">
  <div class="sidebar-item-container">
  <a href="./physics.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Physics background</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container">
  <a href="./stat-fundamentals.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Probability and Statistics, in theory</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container">
  <a href="./stat-practical.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Probability and Statistics, in practice</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container">
  <a href="./diffprog.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Gradient descent</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container">
  <a href="./autodiff.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Automatic differentiation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container">
  <a href="./ml.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Machine learning</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container">
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">Applications</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a>
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">
          <li class="sidebar-item">
  <div class="sidebar-item-container">
  <a href="./diffprog-hep.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Data Analysis in High-Energy Physics as a Differentiable Program</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container">
  <a href="./flow-interp.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Signal Model Interpolation using Normalizing Flows</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container">
  <a href="./sh.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Search for a heavy scalar particle <span class="math inline">\(X\)</span> decaying to a scalar <span class="math inline">\(S\)</span> and a Higgs boson, with final state <span class="math inline">\(b\bar{b}\gamma\gamma\)</span> in the ATLAS detector</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container">
  <a href="./references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container">
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">Appendices</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a>
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">
          <li class="sidebar-item">
  <div class="sidebar-item-container">
  <a href="./neos-extra.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Results when optimizing a neural network observable and binning simultaneously</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>

  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="toc-section-number">4.1</span>  Introduction</a>
  <ul class="collapse">
  <li><a href="#example-maximum-likelihood-estimation" id="toc-example-maximum-likelihood-estimation" class="nav-link" data-scroll-target="#example-maximum-likelihood-estimation"><span class="toc-section-number">4.1.1</span>  Example: maximum likelihood estimation</a></li>
  </ul></li>
  <li><a href="#mini-batching-and-stochastic-gradients" id="toc-mini-batching-and-stochastic-gradients" class="nav-link" data-scroll-target="#mini-batching-and-stochastic-gradients"><span class="toc-section-number">4.2</span>  Mini-batching and stochastic gradients</a></li>
  <li><a href="#speeding-up-convergence-different-optimizer-types" id="toc-speeding-up-convergence-different-optimizer-types" class="nav-link" data-scroll-target="#speeding-up-convergence-different-optimizer-types"><span class="toc-section-number">4.3</span>  Speeding up convergence: different optimizer types</a>
  <ul class="collapse">
  <li><a href="#momentum" id="toc-momentum" class="nav-link" data-scroll-target="#momentum"><span class="toc-section-number">4.3.1</span>  Momentum</a></li>
  <li><a href="#adagrad" id="toc-adagrad" class="nav-link" data-scroll-target="#adagrad"><span class="toc-section-number">4.3.2</span>  Adagrad</a></li>
  <li><a href="#adadeltarmsprop" id="toc-adadeltarmsprop" class="nav-link" data-scroll-target="#adadeltarmsprop"><span class="toc-section-number">4.3.3</span>  Adadelta/RMSProp</a></li>
  <li><a href="#adam" id="toc-adam" class="nav-link" data-scroll-target="#adam"><span class="toc-section-number">4.3.4</span>  Adam</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-gradient-descent" class="quarto-section-identifier d-none d-lg-block"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Gradient descent</span></span></h1>
</div>



<div class="quarto-title-meta">



  </div>


</header>

<section id="introduction" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">4.1</span> Introduction</h2>
<p>Moving forward with the groundwork from the previous section on automatic differentiation, we’ll dive into how this enables a particular learning procedure called <em>gradient descent</em>. We’ll explore what that means, then apply this to a few different scenarios to demonstrate its utility, along with highlighting some more intelligent versions that have arisen as a by-product of the recent deep learning revolution. Importantly, this will also set the stage to show the main mechanism for how neural networks are able to train.</p>
<p>Say we have a ball on a hill, and we wanted to roll it down. How would you go about this task? Well, I imagine you’d probably just give it some initial kick, and then let gravity do the rest. Alright then – let’s take away gravity. What then? Look for the bottom of the hill and try to push it there? But what if you can’t see the hill itself? This is the situation we find ourself in when trying to optimize a workflow. We’re able to see what point we’re at during optimization, but we don’t know where the bottom of the hill is (minimum of our objective), or what the surrounding landscape looks like (could be very expensive to scan over the space).</p>
<p>Ah, but here’s a trick: if we can see where we are at one point on the hill, we can determine the way the slope goes that we’re standing on. If the slope points up, we’ll want to push the ball in the opposite direction to go down. Then, we’re guaranteed to at least get <em>nearer</em> the bottom of the hill. But by how much do we push the ball?</p>
<p>If the ball is close to the bottom of the hill, you could imagine that the ground starts getting flatter, which would make the magnitude of the slope less. Likewise, if the slope is steep, we know we’re probably not at the bottom yet, so we should move a reasonable amount. We can then just move <em>proportionally to the magnitude of the slope.</em></p>
<p>To write this out in equation form: given the horizontal position on the hill <span class="math inline">\(x_i\)</span>, and the vertical position <span class="math inline">\(y_i\)</span>, we propose to move to a new point <span class="math inline">\(x_{i+1}\)</span> in proportion to the slope at the point that we’re on, i.e.</p>
<p><span class="math display">\[
x_{i+1} = x_i - \alpha \frac{\Delta y_i}{\Delta x_i}~,
\]</span></p>
<p>where <span class="math inline">\(\alpha\)</span> is the constant of proportionality that we’re free to choose (also called the <strong>learning rate</strong>, since it modifies the step size we take), and <span class="math inline">\(\Delta\)</span> is a small change that we assume we can calculate. Take note of the minus sign – this is us trying to move in the <em>opposite direction to the slope</em> at <span class="math inline">\(x_i\)</span>.</p>
<p>Instead of assuming we can numerically calculate the small change <span class="math inline">\(\Delta\)</span>, we actually already have a way to calculate a small change at a point from calculus: the derivative! So if we take <span class="math inline">\(y\)</span> as a function of <span class="math inline">\(x\)</span>, then we can replace the deltas with the gradient of <span class="math inline">\(y(x)\)</span> evaluated at our current point <span class="math inline">\(x_i\)</span>, leaving us with</p>
<p><span id="eq-grad-descent"><span class="math display">\[
x_{i+1} = x_i - \alpha \frac{\partial y(x_i)}{\partial x_i}~.
\tag{4.1}\]</span></span></p>
<p><a href="#eq-grad-descent">Equation&nbsp;<span>4.2</span></a> is the equation for gradient descent, and is probably the most important equation in the whole thesis, since it enables all of the downstream applications I’ll talk about later. It’s also the mechanism that enables most artificial intelligence systems to learn.</p>
<p>To reframe this equation one more time through the lens of optimization: given an <strong>objective function</strong> or <strong>loss function</strong> <span class="math inline">\(L\)</span> with parameters <span class="math inline">\(\varphi\)</span> that defines a goal relative to data <span class="math inline">\(d\)</span> (so <span class="math inline">\(L=L(\varphi, x)\)</span>), we can use <em>gradient descent</em> to update the parameters <span class="math inline">\(\varphi\)</span> such that we minimize the objective evaluated at <span class="math inline">\(d\)</span>:</p>
<p><span id="eq-grad-descent"><span class="math display">\[
\varphi_{i+1} = \varphi_i - \alpha \frac{\partial L(\varphi_i, d)}{\partial \varphi_i}~.
\tag{4.2}\]</span></span></p>
<p>See a pictorial demonstration of this rule in <a href="#fig-ball-rolling">Figure&nbsp;<span>4.1</span></a>.</p>
<div id="fig-ball-rolling" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="images/gdesc3.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;4.1: Gradient descent is more easily digestible when considering it as rolling a ball down a hill, but you need to play the role of gravity.</figcaption><p></p>
</figure>
</div>
<p>We’re now specifying the data <span class="math inline">\(d\)</span> because we want to give meaning to the vertical position in the hill: it’s some quantity that’s assessing the quality of our workflow with respect to some data from the real world <span class="math inline">\(d\)</span>, given that we’re using parameters <span class="math inline">\(\varphi_i\)</span>. In practice this may be a small subset of the dataset that we draw at random, since we can’t computationally do this update on the whole data set due to memory restrictions on the device we run this on.</p>
<p>A key point to highlight: this mechanism only works if <span class="math inline">\(L\)</span> is differentiable with respect to <span class="math inline">\(\varphi\)</span>, otherwise we won’t be able to calculate its gradient. This restriction may look tame for common objectives, which tend to involve simple algebraic expressions involving the output of a neural network, which is already a differentiable function. However, if we want to add domain knowledge to our loss function, we may end up discovering that not everything we want to calculate has a well-defined gradient. There are various ways to get around this, including the use of surrogate operations that are <strong>relaxed</strong> (jargon for differentiable) versions of their non-differentiable counterparts. We’ll look at this in more detail when we study applications of this nature.</p>
<p>Let’s look at an example of gradient descent in action.</p>
<section id="example-maximum-likelihood-estimation" class="level3" data-number="4.1.1">
<h3 data-number="4.1.1" class="anchored" data-anchor-id="example-maximum-likelihood-estimation"><span class="header-section-number">4.1.1</span> Example: maximum likelihood estimation</h3>
<p>Say we have some example data drawn from a bi-modal probability distribution that’s somewhat normal-looking, like in <a href="#fig-bidata">Figure&nbsp;<span>4.2</span></a>. We may want to try and model this with a normal distribution, but how do we choose the parameters? We can fit them to the data using maximum likelihood estimation, discussed further in <a href="stat-fundamentals.html#sec-mle"><span>Section&nbsp;2.3.1</span></a>. The basic idea is that, given a probability distribution <span class="math inline">\(p\)</span> with parameters <span class="math inline">\(\mu\)</span>, we want to calculate <span class="math display">\[ \hat{\mu} = \underset{\mu}{\mathrm{argmin}} (-2\ln p(x|\mu))~,\]</span></p>
<p>given some observed data <span class="math inline">\(x\)</span>. In other words, we want to find the value of <span class="math inline">\(\mu\)</span> such that we minimize the negative log-likelihood. We can do this via gradient descent!</p>
<p>Using the update rule in <a href="#eq-grad-descent">Equation&nbsp;<span>4.2</span></a> with <span class="math inline">\(L\)</span> as the negative log-likelihood and <span class="math inline">\((\mu, \sigma)\)</span> playing the role of <span class="math inline">\(\varphi\)</span>, we can run gradient descent for some number of steps until we reach a result that converges within some tolerance. We’ll have to pick some initial value to start for each parameter – here we use 1 for each. In the implementation, we’re using the automatic differentiation framework JAX (<span class="citation" data-cites="jax">Bradbury et al. (<a href="references.html#ref-jax" role="doc-biblioref">2018</a>)</span>) to calculate the gradient of the objective (online viewers can expand the code block above the figure to see the details). This gives us a result in <a href="#fig-onenorm">Figure&nbsp;<span>4.3</span></a>, which isn’t particularly great in my opinion.</p>
<div class="cell" data-execution_count="1">
<div class="cell-output cell-output-display">
<div id="fig-bidata" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="diffprog_files/figure-html/fig-bidata-output-1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;4.2: Example data produced using two normal distributions in unequal amounts. One is centered on 3, and the other on 0, both with unit standard deviation.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<div class="cell" data-execution_count="2">
<div class="cell-output cell-output-display">
<div id="fig-onenorm" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="diffprog_files/figure-html/fig-onenorm-output-1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;4.3: A single normal distribution fitted to the data in <a href="#fig-bidata">Figure&nbsp;<span>4.2</span></a> using gradient descent.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>Pretending we didn’t know that the data came from a mixture of normal distributions, we can make a more sophisticated model, e.g.</p>
<p><span class="math display">\[
p(x |\mu_1, \sigma_1, \mu_2, \sigma_2) = \frac{1}{2}\mathrm{Normal}(\mu_1, \sigma_1) + \frac{1}{2}\mathrm{Normal}(\mu_2, \sigma_2)~.
\]</span></p>
<p>We can then simultaneously optimize <span class="math inline">\(\mu_1, \sigma_1, \mu_2, \sigma_2\)</span> in exactly the same way, with no modification to the procedure other than using the new likelihood as the loss function. Doing this yields the distribution in <a href="#fig-twonorm">Figure&nbsp;<span>4.4</span></a>. We can tell that the shape of the distribution is represented better here, which is also indicated by the lower negative log-likelihood than in the case of the single normal distribution. Since we forced the proportions of the mixture to be half and half, the lower second peak is modelled through a wider normal distribution to match the height of the second, smaller mode.</p>
<p>Interestingly, if we use 1 as the init for every parameter, we recover the solution from <a href="#fig-onenorm">Figure&nbsp;<span>4.3</span></a>, so we have to make sure there’s a little mutual variation in the starting values so that the gradients push the different <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> values from different positions, allowing the second mode to be discovered. This demonstrates the behavior of gradient descent to move to some <em>local</em> minimum of the objective, and won’t always converge to something optimal or intuitive.</p>
<div class="cell" data-execution_count="3">
<div class="cell-output cell-output-display">
<div id="fig-twonorm" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="diffprog_files/figure-html/fig-twonorm-output-1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;4.4: A mixture of two normal distributions fitted to the data in <a href="#fig-bidata">Figure&nbsp;<span>4.2</span></a> using gradient descent.</figcaption><p></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="mini-batching-and-stochastic-gradients" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="mini-batching-and-stochastic-gradients"><span class="header-section-number">4.2</span> Mini-batching and stochastic gradients</h2>
<p>In reality, we’re not generally able to update our parameters using gradients computed over all our data at once in one batch, since that data may not fit on the device we’re using to compute the program. We’d like to do this if we truly want to calculate the expectation of the gradient (in practice, the empirical mean) across all the training data we have, which has some convergence guarantees for arriving at a local minimum. Instead, we often split our data up into <strong>minibatches</strong>, which are fixed-size partitions of the data, typically randomly selected<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>. When we have seen enough batches such that we cover all the available training data, that number of steps is known as an <strong>epoch</strong>. This gives rise to the notion of the <strong>batch size</strong> – the size of each partition – which then inherently decides how many steps an epoch will take (smaller batch size = more steps and vice-versa).</p>
<p>An extreme version of splitting up into batches is <strong>stochastic gradient descent</strong>, which is just setting the batch size to 1. Gradients computed in this way will have very high variance as the name suggests, since parameter updates would be based on the performance on each individual training example. Surprisingly though, this high variance of the gradients can often help when wanting to explore the loss landscape more, and may help in jumping to new local minima that batch gradient descent may ignore.</p>
</section>
<section id="speeding-up-convergence-different-optimizer-types" class="level2" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="speeding-up-convergence-different-optimizer-types"><span class="header-section-number">4.3</span> Speeding up convergence: different optimizer types</h2>
<p>To improve upon the rate of convergence in minibatch gradient descent, several variants have been proposed that incorporate additional information to make the gradient update. The methods are generally referred to as <strong>optimizers</strong> – we’ll cover just a couple of the more common ones here.</p>
<section id="momentum" class="level3" data-number="4.3.1">
<h3 data-number="4.3.1" class="anchored" data-anchor-id="momentum"><span class="header-section-number">4.3.1</span> Momentum</h3>
<p>What happens when we’re in a ravine during optimization, i.e.&nbsp;a local minimum with two steep slopes on either side in one dimension? Without modification, gradient descent will typically bounce between the ravines: we’re moving in the downward direction, and with a larger step size since the slope is steep. How may we help this converge easier? Well, we could of course modify the learning rate in an adaptive fashion to try to dampen this oscillation about a minimum in a ravine. Something else that is done is to incorporate some amount (<span class="math inline">\(\gamma\)</span>) of the <em>previous update step</em>, which we can write with a modified update rule through a placeholder variable <span class="math inline">\(v\)</span>, defined recursively:</p>
<p><span class="math display">\[
v_{i} = \gamma v_{i-1} + \alpha \frac{\partial L(\varphi_i, d)}{\partial \varphi_i};~~~ \varphi_{i+1} = \varphi_i - v_i~.
\]</span></p>
<p>To give some intuition as to what’s happening here, we note that there’s still a standard gradient update rule in there, but we’re also “speeding up” the update by an amount <span class="math inline">\(\gamma v_{i-1}\)</span>, meaning if we did a large update previously, then we’ll go even further this time. The analogy here is really still a ball rolling down a hill, but we’re compounding speed just like gravity would make us do in real life. Moreover, if the previous update had a different sign for the gradient (travelling in the opposite direction), we’ll actually be <em>slowed</em> by <span class="math inline">\(\gamma v_{i-1}\)</span>, which will prevent these types of oscillating situations bouncing between two steep slopes. This is termed gradient descent with <strong>momentum</strong>; I don’t know if this was where it was originally proposed, but many seem to cite <span class="citation" data-cites="momentum">(<a href="references.html#ref-momentum" role="doc-biblioref">Qian 1999</a>)</span>, despite it opening with “A momentum term is usually used in…”.</p>
</section>
<section id="adagrad" class="level3" data-number="4.3.2">
<h3 data-number="4.3.2" class="anchored" data-anchor-id="adagrad"><span class="header-section-number">4.3.2</span> Adagrad</h3>
<p>We return to the idea of <em>adaptive learning rates</em>. In the multi-dimensional parameter setting (could even be billions if we’re looking at neural networks), some parameters will have higher importance than others, i.e.&nbsp;the loss landscape could be flat in some dimensions, and convex in others. However, there could be parameters that are locally flat in some region, but have more interesting structure at a distance far from that region, or perhaps at least have structure that is more widely spread out. This could happen if that parameter is responsible for information relating to a sparse feature, for instance. If the learning rate makes the stepsize small in comparison to this distance, then we’ll never explore that structure well. This then gives rise to the notion of adapting the learning rate on a per-parameter basis to aid more efficient exploration of the loss landscape in search of a local minimum.</p>
<p>Adagrad (shorthand for “adaptive gradient”) is a method that endeavors to tackle this issue. It scales the learning rate for each parameter in such a way as to increase the step size for parameters in which we have moved less, and decrease it for those in which we have moved more. Specifically, it scales the learning rate by the inverse of the sum of the (squared) gradients. Denoting the gradient of one example parameter <span class="math inline">\(\varphi^{(1)}\)</span> at step <span class="math inline">\(i\)</span> as <span class="math inline">\(g^{(1)}_{i}\)</span>, we can write the update step for that parameter as</p>
<p><span class="math display">\[
\varphi^{(1)}_{i+1} = \varphi^{(1)}_{i} + \Delta\varphi_i^{(1)};~~~ \Delta\varphi_{i}^{(1)} = - \frac{\alpha}{\sqrt{\sum^{i}_{j=0}\left(g^{(1)}_{j}\right)^2 + \epsilon}} g^{(1)}_{i} ~,
\]</span></p>
<p>where <span class="math inline">\(\epsilon\)</span> is a very small number just to prevent dividing by zero. We’ve defined the convenient notation of <span class="math inline">\(\Delta\varphi\)</span>, which becomes the subject of modification for the methods hereafter. For <span class="math inline">\(n\)</span> parameters, we denote <span class="math inline">\(g_{i}\)</span> as the vector of all gradients <span class="math inline">\([g^{(1)}_{i}, \dots, g^{(n)}_{i}]\)</span>. Then, if we treat all the operations as <em>elementwise</em>, i.e.&nbsp;as one may expect with <code>numpy</code> arrays, we can write this again more compactly for all parameters <span class="math inline">\(\varphi\)</span> as</p>
<p><span class="math display">\[
\varphi_{i+1} = \varphi_{i} + \Delta \varphi_i ;~~~ \Delta\varphi_i = - \frac{\alpha}{\sqrt{\sum^{i}_{j=0}g^2_{j} + \epsilon}} g_{i} ~.
\]</span></p>
<p>One of the main reasons for the appeal in Adagrad is that you don’t really need to tune the scalar learning rate <span class="math inline">\(\alpha\)</span> from it’s initial value, since it’s being done on-the-fly for each parameter. However, an issue arises in that the sum <span class="math inline">\(\sum^{i}_{j=0}g^2_{j}\)</span> is one of positive numbers, and shall grow without bound, which will eventually reduce the learning rate to a crawl and stop learning altogether. Avoiding this is the groundwork for our next optimizers: Adadelta and RMSProp.</p>
</section>
<section id="adadeltarmsprop" class="level3" data-number="4.3.3">
<h3 data-number="4.3.3" class="anchored" data-anchor-id="adadeltarmsprop"><span class="header-section-number">4.3.3</span> Adadelta/RMSProp</h3>
<p>As mentioned, tackling the growing sum of squared gradients is the purpose behind Adadelta <span class="citation" data-cites="adadelta">(<a href="references.html#ref-adadelta" role="doc-biblioref">Zeiler 2012</a>)</span> and RMSProp <span class="citation" data-cites="rmsprop">(<a href="references.html#ref-rmsprop" role="doc-biblioref">Hinton 2018</a>)</span>, where we’ll start with the former.</p>
<p>Instead of maintaining a sum over the gradients <span class="math inline">\(g_i^2\)</span> up to step <span class="math inline">\(i\)</span>, we recursively define a <em>decaying average</em> <span class="math inline">\(E\left[g^2\right]_i\)</span>:</p>
<p><span class="math display">\[
E\left[g^2\right]_i=\gamma E\left[g^2\right]_{i-1}+(1-\gamma) g_i^2~,
\]</span></p>
<p>where we proportionally weight the previous average and the current squared gradient, each by a factor of <span class="math inline">\(\gamma\)</span> and <span class="math inline">\(1-\gamma\)</span> respectively. This is a similar idea to momentum: we’re “remembering” the previous steps in some manner that we can tune numerically through <span class="math inline">\(\gamma\)</span>. This leads to a <span class="math inline">\(\Delta \varphi_i\)</span> of</p>
<p><span id="eq-adadelta1"><span class="math display">\[
\Delta \varphi_i = - \frac{\alpha}{\sqrt{E\left[g^2\right]_i+ \epsilon}} g_{i} ~.
\tag{4.3}\]</span></span></p>
<p>Coincidentally, this is actually the equation for the <em>RMSProp</em> update, developed independently from Adadelta around the same time! For Adadelta though, we’re not quite done yet though; the Adadelta authors noted that this update of <span class="math inline">\(\Delta \varphi_i\)</span> – just like the update in <em>any</em> of the methods above – does not have the same units as the parameters (it’s instead dimensionless), which is not in principle an issue, but maybe better behaviour could be had by updating by a quantity of the same scale as the parameters themselves. This led to defining a second decaying average using the definition of <a href="#eq-adadelta1">Equation&nbsp;<span>4.3</span></a> for <span class="math inline">\(\Delta \varphi_i\)</span>, which we’ll also define recursively:</p>
<p><span class="math display">\[
E\left[\Delta \varphi^2\right]_i=\gamma E\left[\Delta \varphi^2\right]_{i-1}+(1-\gamma) \Delta \varphi_i^2 ~.
\]</span></p>
<p>This expression is then also put under a square root with the same small <span class="math inline">\(\epsilon\)</span>, and we use this in place of the static learning rate <span class="math inline">\(\alpha\)</span>, i.e.&nbsp;replace it with <span class="math inline">\(\sqrt{E\left[\Delta \varphi^2\right]_i + \epsilon}\)</span>. At least, we’d like to – this would lead to some kind of nested recursion relation between <span class="math inline">\(E\left[\Delta \varphi^2\right]_i\)</span> and <span class="math inline">\(\Delta \varphi_i\)</span>, so we need to instead need to approximate this with a value we have access to. The paper does this with the value from the previous iteration: <span class="math inline">\(E\left[\Delta \varphi^2\right]_{i-1}\)</span>. This at last gives us the full relation for one Adadelta update:</p>
<p><span id="eq-adadelta"><span class="math display">\[
\Delta \varphi_i = - \frac{\sqrt{E\left[\Delta \varphi^2\right]_{i-1} + \epsilon}}{\sqrt{E\left[g^2\right]_i+ \epsilon}} g_{i} ~.
\tag{4.4}\]</span></span></p>
<p>A curious thing to note in <a href="#eq-adadelta">Equation&nbsp;<span>4.4</span></a> is the lack of <span class="math inline">\(\alpha\)</span> – we’ve removed the scalar learning rate altogether, and instead only need to specify the decay factor <span class="math inline">\(\gamma\)</span> before undergoing optimization.</p>
</section>
<section id="adam" class="level3" data-number="4.3.4">
<h3 data-number="4.3.4" class="anchored" data-anchor-id="adam"><span class="header-section-number">4.3.4</span> Adam</h3>
<p>We’ve reached <strong>Adam</strong> <span class="citation" data-cites="adam">(<a href="references.html#ref-adam" role="doc-biblioref">Kingma and Ba 2014</a>)</span>: by far the most common default choice of optimizer for deep learning, and the one frequently used within this thesis. Adam draws upon ideas from all of the above methods; it keeps track of both a decaying average of gradients <em>and</em> squared gradients. We can write their forms explicitly:</p>
<p><span class="math display">\[
\begin{aligned}
m_i &amp;=\beta_1 m_{i-1}+\left(1-\beta_1\right) g_i \\
v_i &amp;=\beta_2 v_{i-1}+\left(1-\beta_2\right) g_i^2~,
\end{aligned}
\]</span> where we’ve introduced separate decay factors <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span> for each quantity. If you remember earlier, we were using <span class="math inline">\(E[g]_i\)</span> notation for these types of quantities, and that’s not a coincidence – they’re moving averages, and so are Monte Carlo estimations of expectation values (albeit off by decay terms etc). Specifically, <span class="math inline">\(m_i\)</span> and <span class="math inline">\(v_i\)</span> are estimators of the first moment (mean) and second moment (variance) of the gradients <span class="math inline">\(g_i\)</span>, hence their variable names, and the name of Adam (adaptive <em>moment</em> estimation).</p>
<p>One thing the Adam authors note is the fact that they used the algorithm through initializing <span class="math inline">\(m_i\)</span> and <span class="math inline">\(v_i\)</span> as vectors of zeroes, and then found that the resulting estimates of the gradient moments were biased towards zero. The paper includes a brief derivation of the actual value of these moments, and shows that in either case of <span class="math inline">\(m_i\)</span> or <span class="math inline">\(v_i\)</span>, you can correct for this bias by dividing through by <span class="math inline">\((1-\beta^i)\)</span>, where we explicitly raise <span class="math inline">\(\beta\)</span> to the power of the iteration number <span class="math inline">\(i\)</span> (the paper uses <span class="math inline">\(t\)</span> instead). This results in the bias-corrected version of the previous formula: <span class="math display">\[
\begin{aligned}
\hat{m}_i &amp;=\frac{m_i}{1-\beta_1^i} \\
\hat{v}_i &amp;=\frac{v_i}{1-\beta_2^i}~.
\end{aligned}
\]</span> We then use these bias-corrected moment estimates to construct an expression for the Adam update, which looks a lot like that for RMSProp in <a href="#eq-adadelta1">Equation&nbsp;<span>4.3</span></a>:</p>
<p><span class="math display">\[
\Delta \varphi_i = - \frac{\alpha}{\sqrt{\hat{v}_i}+ \epsilon} \hat{m}_{i} ~.
\]</span></p>
<p>As you can see, gradient descent can get pretty complicated! The reason I went through this explicitly was to show that gradient descent has been extremely well-studied during the rise of deep learning, and we often need to go beyond simple update rules in order to effectively learn the properties we want from our parameters in practice. In fact, the main work you’ll see in the first application later on initially wouldn’t learn at all until we switched to using Adam!</p>


<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography" style="display: none">
<div id="ref-jax" class="csl-entry" role="doc-biblioentry">
Bradbury, James, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, et al. 2018. <em><span>JAX</span>: Composable Transformations of <span>P</span>ython+<span>N</span>um<span>P</span>y Programs</em> (version 0.2.5). <a href="http://github.com/google/jax">http://github.com/google/jax</a>.
</div>
<div id="ref-rmsprop" class="csl-entry" role="doc-biblioentry">
Hinton, Geoffrey. 2018. <span>“Neural Networks for Machine Learning Lecture 6a: Overview of Mini-Batch Gradient Descent.”</span> <a href="https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf">https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf</a>.
</div>
<div id="ref-adam" class="csl-entry" role="doc-biblioentry">
Kingma, Diederik P., and Jimmy Ba. 2014. <span>“Adam: A Method for Stochastic Optimization.”</span> arXiv. <a href="https://doi.org/10.48550/ARXIV.1412.6980">https://doi.org/10.48550/ARXIV.1412.6980</a>.
</div>
<div id="ref-momentum" class="csl-entry" role="doc-biblioentry">
Qian, Ning. 1999. <span>“On the Momentum Term in Gradient Descent Learning Algorithms.”</span> <em>Neural Networks</em> 12 (1): 145–51. https://doi.org/<a href="https://doi.org/10.1016/S0893-6080(98)00116-6">https://doi.org/10.1016/S0893-6080(98)00116-6</a>.
</div>
<div id="ref-adadelta" class="csl-entry" role="doc-biblioentry">
Zeiler, Matthew D. 2012. <span>“<span>ADADELTA:</span> An Adaptive Learning Rate Method.”</span> <em>CoRR</em> abs/1212.5701. <a href="http://arxiv.org/abs/1212.5701">http://arxiv.org/abs/1212.5701</a>.
</div>
</div>
</section>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>There’s a funny issue to do with the notion of randomly selecting here; if we select <em>with replacement</em>, then we’re sampling i.i.d. uniform random variables. This makes the theory of analyzing the performance much neater. But in practice, we never do this, and just shuffle the data and stream it, making it non-i.i.d, and therefore more difficult to analyze.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config);
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./stat-practical.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Probability and Statistics, in practice</span></span>
      </a>
  </div>
  <div class="nav-page nav-page-next">
      <a href="./autodiff.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Automatic differentiation</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>
