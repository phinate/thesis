---
pdf-engine: pdflatex
format:
  html:
    code-fold: true
    default-image-extension: svg
  pdf:
    default-image-extension: pdf
jupyter: python3
---
# Search for a heavy scalar particle $X$ decaying to a scalar $S$ and a Higgs boson, with final state $b\bar{b}\gamma\gamma$ in the ATLAS detector {#sec-anal}

As part of my PhD, I made some various contributions to a search for a physics process $X\rightarrow SH \rightarrow b\bar{b}\gamma\gamma$, where $X$ and $S$ are scalar particles beyond the Standard Model. My contributions include:

- building a faster framework for data processing
- assisting with the development of a pipeline to classify signal versus background using a neural network parametrized in the truth-level masses $m_X$ and $m_S$
- using the method I developed with normalizing flows to interpolate a 2-D signal shape across different signal mass points
- various small contributions to the statistical analysis


The analysis of this data is still in the preliminary stages at the time of writing, so only initial work will be shown. Moreover, as this is a thesis on my personal work and not a supporting note, only the minimum required information about the analysis will be provided in order to solely aid the understanding of the applications I contributed to.


## Overview and motivation

The new particle discovered in 2012 @higgs is widely regarded to represent the Higgs boson as predicted by the Standard Model, which is predicted to have a rest mass of 125GeV. While this is confirmatory of the existence of some flavor of the Standard Model, we know this isn't the end of the story. Many physics theories beyond the Standard Model predict some kind of Higgs sector, where additional scalar bosons could exist and interact with the Higgs in some way, but have yet to be discovered. To this effect, it is useful to probe data taken from proton-proton collisions to see if particles produce states that correspond to e.g. resonances that look like these new scalars.

In particular, this work looks at the process $X\rightarrow SH \rightarrow b\bar{b}\gamma\gamma$, where $X$ is a heavy scalar with a mass large enough to produce a Higgs with Standard Model mass (i.e. $m_H = 125 \text{ GeV}$) and a scalar $S$ with mass $m_S$ such that $m_X > m_S + m_H$, which makes the decay $X\rightarrow SH$ satisfy energy conservation. This particular process is predicted by a number of different theories, including but not limited to:

- Next-to-Minimal 2HDM @N2HDM
- Next-to-Minimal Supersymmetric Standard Model @NMSSM
- Complex 2HDM model @C2HDM
- Two-Real-Scalar SM extension @TRSM

It's worth noting the problems that these theories solve (i.e. why they are interesting at all), which span combatting matter-antimatter asymmetry mechanisms in the early universe (@asym1, @asym2) and producing possible dark matter candidates (@dm1, @dm2) amongst other things.

The next thing to address is why it's worth it to look at $b\bar{b}\gamma\gamma$ as a final state. The first reason is that if we assign the di-photon state as originating from a Higgs boson, we would have a a very clear way to select events we want to analyze, as experiments at the Large Hadron Collider have the ability to precisely determine the energies of photons in their electromagnetic calorimeters. Events originating from decays of interest would then leave a fairly sharp peak at 125 GeV in the invariant mass spectrum of the two-photon system, which we can turn into a requirement for including those events in our analysis. This would leave the $b$ quarks (which would be seen in the detector as jets due to hadronization) as originating from the $S$ particle, which is the dominant decay mode of the $S$ when one assumes that it has properties similar to the Standard Model Higgs. Of course, despite all this, choosing any one particular final state is a needle-in-a-high-dimensional-haystack approach, with there being many other choices of theories and states to consider. In that sense, there is no particular reason other than to look where has not been checked yet.

What exactly are we looking for then? As alluded to in @sec-asymptotics, searches for new particles involve calculating $p$-values (or $\text{CL}_s$ values), either trying to reject a background-only hypothesis for discovery, or to set an upper limit on the signal strength. We concentrate on the latter, with the steps ahead demonstrating the task of the analyzer to optimize the workflow for this purpose.

## The ATLAS detector

## Simulated data

Data for the signal process of $X\rightarrow SH \rightarrow b\bar{b}\gamma\gamma$, where the $b$ quarks are associated to the $S$ and the photons are associated to the Higgs boson, was generated at a variety of possible combinations of $m_X$ and $m_S$. We can see the points chosen in @fig-mass-grid, where the upper-left quadrant is kinematically forbidden (there, $m_X < m_S + m_H$).

![A grid of the different points for which data originating from the signal process was simulated.](images/sh/mass-grid){#fig-mass-grid}


## Preprocessing and selection

## Fit strategies {#sec-fit-strategies}

A few strategies were investigated to see which variables provided the most discriminating power when comparing their distribution in signal and in background. The reason for this is that we want to pick a *summary statistic* of the data to use as a foundation for the HistFactory statistical model construction (@sec-hifa), and choosing something that differs in a significant way across signal and background distributions should hopefully give us smaller $p$-values and stronger limits.

We go over the two main approaches explored in the following sections.

### 2-D fit in the $m_{bb\gamma\gamma}$ and $m_{bb}$ plane

A somewhat natural choice to look at is the variables that, when constructed, recover the mass resonance for each of the proposed new particles $X$ and $S$. We thus look at the invariant mass of the overall final state $m_{bb\gamma\gamma}$ (which should peak at the chosen value of $X$) and the invariant mass of the $b$-tagged jets $m_{bb}$ (which should peak at the chosen value of $S$). Some example plots for the shape of these distributions in the signal Monte-Carlo samples can be found in @fig-signal-examples.

![Some example distributions of the $m_{bb\gamma\gamma}$ and $m_{bb}$ invariant mass plane.](images/sh/){#fig-signal-examples}



### 1-D fit using a parametrized neural network-based summary statistic

You may have thought based on earlier discussion that it may not be the case that a one-size-fits-all variable (or variables) exists across every combination of $m_X$ and $m_S$ featured in @fig-mass-grid, or indeed across any possible combination of hypothesized masses. It is then of interest to look for an observable that can be *parametrized* in the truth masses $m_X$ and $m_S$ such that the chosen observable can adapt to provide better results depending on the hypothesis we're probing. An example of such an observable is the so-called **parametrized neural network** (pNN), originally proposed in @pnn.

We're interested in neural networks more generally for their ability to provide flexible outputs (here, 1-D) that can learn to perform as we train them to do so, e.g. to discriminate strongly between events coming from signal and background respectively. The way a pNN is structured is no different to that of any other neural network, with the only change being the inclusion of the parameters of your physics model (here, $m_X$ and $m_S$) as additional inputs. This aims to essentially use those inputs as choosing the best model for the use case; when comparing different values of the inputs, the information from one set of values will likely be propagated through the network in a very different way to the other set, which can provide information about the context in which the network is performing inference in. Training a pNN is then no different to standard procedures -- by providing the additional inputs within a batch, the loss structure of choice doesn't have to change in any way to accommodate this, but will optimize for good average performance across all provided contexts. Moreover, if the network is able to infer this context well from the provided physics parameters, the performance of the network should carry over to new, unseen parameter points (which for us would be the gaps between the points in @fig-mass-grid).

#### Practical training considerations {-}

When training this model for our use case, a number of practical issues arose. The first is that we need to provide a set of signal truth masses for *all* events, including those coming from background. Of course, these labels do not exist; following the approach in @pnn, we then circumvented this to some degree by uniformly sampling these labels from the distribution present in the signal events, and assigning these as the labels to background events. The rationale behind this is to try and encode as little additional information as possible, hoping that the network will still be able to pick up the distribution of the signal context instead. Another issue is that of scaling the input variables, with it being common practice to scale all inputs to either reside in the range [0,1], or to have zero mean and unit variance. This scaling also needs to be applied to the truth masses; depending on the method chosen, the distribution of the masses could become very skewed to 0, which has the potential to make it more difficult to encode the context information for having very low values numerically.

Many combinations of input variables were tried, with some examples being kinematics like $p_T$, $\eta$, $\phi$ (for the 2 photons and the 2 jets with highest $p_T$), differences in these variables such as $\Delta\phi(\gamma_1, \gamma_2)$, $\Delta\eta(\gamma_1, \gamma_2)$, and also non-linear quantities like the angular distance between objects $\Delta R = \sqrt{\Delta\phi + \Delta\eta}$. Despite all this, the most effective training input configuration was found to just be the invariant masses $m_{bb\gamma\gamma}$ and $m_{bb}$, which speaks to the power of the 2-D fit approach previously mentioned.



## Interpolation between signal shapes {#sec-sh-flows}

For either strategy in @sec-fit-strategies, we're interested in some kind of way to make inference about points in-between those we've already simulated in @fig-mass-grid. We would of course like to just simulate points on the fly for any hypothesis we want to probe, but that's time, compute, and CO$_2$ that we'd rather not have in excess. In order to do this simulation-free, we need to produce the expected shape of the counts from simulation of that signal process. We're now in the regime of somehow trying to *interpolate* the shape of the signal in whatever variables we want to use as input to the HistFactory model. Moreover, it would be great if this could come with a notion of uncertainty; by adding this uncertainty to our statistical model, we'll be able to quantify the fact that we're going to have a worse result in the regions for which we didn't simulate data directly.

This is exactly the context in which I developed the method presented in [Chapter -@sec-flow-interp]! I use **normalizing flows** that are conditioned on the truth masses $m_X$ and $m_S$ as a mechanism to interpolate between the signal shapes used for fitting. While this was initially designed as a strategy for the 2-D fit of $m_{bb\gamma\gamma}$ and $m_{bb}$, where samples from the flow would be drawn from the joint conditional distribution $p(m_{bb\gamma\gamma} m_{bb} | m_X, m_S)$ and histograms of those samples made for use as 2-D templates, the method works equally well for the 1-D pNN strategy. The reason for this is that we can just draw samples in the same way, and then build up the shape of the pNN output by computing the results given the flow samples, since the flow is defined over the same set of input variables $m_{bb\gamma\gamma}$ and $m_{bb}$. We can then normalize this histogram to the appropriate amount, provided we have access to that factor (more on that later; we have to interpolate that too).

Exactly like in [Chapter -@sec-flow-interp], a train/test split is made across a number of $(m_X, m_S)$ points, which can be seen in @fig-train-test-sh. Here, we don't make any separate valid split that involves the training grid data.

![The train/test split across the grid of the different points for which signal process data was simulated.](images/sh/flow-mass-grid){#fig-train-test-sh}

The training setup is then to look at event-wise pairs of $m_{bb\gamma\gamma}$ and $m_{bb}$, along with the context label $(m_X, m_S)$ representing the parameters at which the event was generated. These will be provided in batches, and used as input to the flow, which assesses its quality though calculating the mean negative log-likelihood across the batch, and then uses the gradient of this to update it's transform parameters. The results from this training procedure are the continuously defined conditional distribution $p(m_{bb\gamma\gamma} m_{bb} | m_X, m_S)$, as well as the ability to draw samples from this distribution.

One practical aspect of this training procedure that I didn't cover in[Chapter -@sec-flow-interp] is that of *event weights*. When applying this problem to a particle physics context, we're conscious of the fact that events are not supplied alone, but also with a weight that controls its relevance as a proportion of the distributions defined by that physics process. This means that the distributions we're trying to imitate with the flow need to be appropriately scaled according to these weights. A fairly simple way to do this is to just weight the contribution to the loss from a given event proportional to it's event weight, i.e. *multiply the loss of an event (the negative log-likelihood) by its weight*. The intuition behind this is that one would expect an event with weight 2 to have the contribution to the distribution of two individual events with identical $m_{bb\gamma\gamma}$ and $m_{bb}$ values. It follows that the loss expected from those two events is just two equal contributions of loss at the same values of $m_{bb\gamma\gamma}$ and $m_{bb}$.

### Results

I performed the training procedure as described for 5 flows with different initializations, and use their ensemble as the predictive model, with the standard deviations of these predictions (for likelihood values or histogram yields) acting as an uncertainty. We can see some of the resulting distributions for the test context points in @fig-flow-vs-data-sh.

![Example shapes of the distributions for the test set, both from the flow and from data.](images/sh/data-vs-flow){#fig-flow-vs-data-sh}

We then would like to see what happens when we incorporate the flow uncertainties. For this, we'll look at the *binwise pull values* just as in[Chapter -@sec-flow-interp], i.e.

$$
\text{pull } = \frac{\text{flow prediction }-\text{ data prediction}}{\sigma_{\text{flow}}}~.
$$ {#eq-pull2}


We'll start by inspecting this bin-by-bin for histograms with bins of size 20 GeV in both directions, which we can see for the training set in @fig-pulls-train-sh and the test set in @fig-pulls-test-sh, where the histograms are formed 300 GeV either side of the mass value in both directions where the majority of the distribution lies.

![Plots of the bin-by-bin pull across different context points from the training set. The axes are omitted for legibility, but the histogram range is +/- 300 GeV in both directions from the red cross (without including negative values).](images/sh/sh-train-diff){#fig-pulls-train-sh}

![Plots of the bin-by-bin pull across different context points from the test set. The axes are omitted for legibility, but the histogram range is +/- 300 GeV in both directions from the red cross (without including negative values).](images/sh/sh-test-diff){#fig-pulls-test-sh}

We can then have a look at the overall performance by making histograms of the pull accumulated over all training and test points separately, but will make sure to filter out the empty bins by thresholding the histograms such that neither makes a prediction of below 1 count in any given bin, else we could artificially inflate the performance of the flow when it's good at predicting values far from the bulk of the distribution. These plots are shown in @fig-pulls-sh.

![Left: Histogram of the bin-by-bin pull accumulated across different context points from the training set. Right: Same for the test set. Both plots have a normal distribution fit overlayed.](images/sh/sh-pulls){#fig-pulls-sh}

## Interpolation between overall yields

In addition to the method presented in @sec-sh-flows, there's a missing component: the overall normalization of the produced histograms. This is important from the perspective of interpretation of the signal strength $\mu$, which represents the relative overall cross-section of $X\rightarrow SH$ multiplied by the branching ratios for $S \rightarrow bb$ and $H \rightarrow \gamma\gamma$. Why? Consider that events are generated with some assumed cross-section, and that information is encoded in the event weights. If we do not provide this information as a normalization when inputting predicted counts from simulation into our statistical model, then the resulting signal strength limits will not accurately reflect the cross-section used, since the counts are incorrectly normalized. We then desire a second, simpler interpolation across the space of the total event count per process, which is the *sum of the event weights*.

### Gaussian process interpolation

A flexible class of models that come with a notion of uncertainty are **Gaussian processes** (GPs). I include only a brief description; a working knowledge of GPs is not needed to gauge the quality of the results. To describe how we'd use GPs in this case, give each member the set of signal mass grid points ($m_X, m_S$ values) a corresponding label for the sum of the event weights ($\sum_i w_i$) produced for that mass tuple. Each of these $m_X, m_S$ values represents a random variable that we can model as coming from a normal distribution, where the means are often either zero or centered on the data, and the covariances are determined from a functional form which is called the *kernel*. The kernel is where the magic happens in GPs, and acts as the prior in a Bayesian inference context.

Speaking of inference, we're then interested in inferring the values of $\sum_i w_i$ for new, unseen pairs of $m_X, m_S$ values. By treating the joint distribution of the test and training points as a multivariate normal, with dimensions equal to the size of each set of points combined, we can condition this distribution on just the training data to get the distribution of the points in test set only. Since normal distributions are closed under conditioning, the resulting distribution will also be a multivariate normal; predictions for these points are then just samples from this distribution, which lends itself naturally to the notion of uncertainty, since we can include intervals of our choosing from that distribution along with the predicted samples.

Applying all this leads to the set of predicted values from this conditional distribution shown in @fig-sh-gp, where the left-hand plot shows the interpolated yield predictions, and the right-hand plot shows the corresponding relative uncertainties. The exact functional form of the kernel was chosen somewhat arbitrarily; I took many combinations of common kernels, and applied k-fold validation (with 10 folds) across the training points, then chose the form that minimized the average absolute error divided by the uncertainty from the GP (c.f. the pull from @eq-pull2). We can see that the bottom-left hand corner is white, which represents the model predicting negative yields. This is not considered problematic, as this region is outside the realm of interpolation, which is the planned context in which this would be used. Moreover, one can notice in the right-hand plot the increase in relative uncertainty as we go to the bottom-left hand corner (to nearly 100% for some values) despite that being a region of high data density. This is attributed to the fact that there is a very rapid variation in the yields in this corner of parameter space, which can be seen in the left-hand plot with the concentration of blue lines.

![Left: Plot of the interpolated yields using a Gaussian process model, with the full post-fit kernel function described in the top of the plot. Right: The map of the corresponding uncertainties for the GP predictions.](images/sh/sh-pulls){#fig-pulls-sh}
