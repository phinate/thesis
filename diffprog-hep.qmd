---
execute:
  echo: false
format:
  html:
    code-fold: true
    default-image-extension: svg
  pdf:
    default-image-extension: pdf
jupyter: python3
---

# Physics Analysis as a Differentiable Program

## Motivation
Given the success of the Standard Model (SM), analysis of data from the LHC usually occurs for two reasons:
- Precisely measuring SM processes to look for small deviations with SM predictions
- Searching for new physics signatures as predicted by models beyond the SM

When analysing data in this way, we'll have lots of free parameters to tune. These can be as simple as a threshold value that you limit the p_T to, or as complicated as the weights and biases that determine a neural network for identifying $b$-jets. We can of course choose any values for these quantities to do our analysis, but the resulting physics that follows may suffer as a result. As such, we're likely to try some kind of optimization to improve the answers to our physics questions. How do we do this in practice?

In either case above, there is a notion of <span style="color:#13becf">signal</span> (what youâ€™re looking for) and <span style="color:#ff7f0e">background</span> (everything else).
Generally, we then try to choose a parameter configuration that can separate (or discriminate) the signal from the background, allowing us to extract just the data we think is relevant to the physics process we're looking at. As an example, machine learning models are often trained using the **binary cross-entropy** loss as an objective, which corresponds to optimizing the ability of the model to identify whether an event originated from signal or background processes. A closely related goal is the **Asimov significance** in the case of signal and background event counts $s$ and $b$ with *no uncertainty* on either quantity. The formula for this stems from assuming a Poisson likelihood function, and is equal to

$$
Z_A = \sqrt{2\sum_{i\in bins}((s_i + b_i)(\log{(1 + s_i / b_i)}) - s_i)}~.
$$ {#eq-asimov-significance}

As indicated in the sum, these counts can be spread across different bins in the case where your data is a histogram, but the formula is more commonly reduced to the 1-bin scenario that just deals with the overall numbers of signal and background events. In this case, we can then Taylor expand the logarithm to get

$$Z_A = \sqrt{2((s+b)(s/b + \mathcal{O}(s/b) - s)} \approx s/\sqrt{b}~~~\mathrm{for}~s<<b.$$

This makes it much clearer to see that optimising with respect to $Z_A$ is just a fancier way of trying to increase the amount of signal compared to the amount of background, which is directly analogous to separating signal from background, just as binary cross-entropy would do.

Now, this is all very sensible of course (we want to discover our signal), but this approach has some shortcomings that distance the efficacy of the resulting configuration from our physics goals. A recent review of deep learning in LHC physics [@deeplhc] lets us in on why:

> (...) tools are often optimized for performance on a particular task that is **several steps removed from the ultimate physical goal** of searching for a new particle or testing a new physical theory.

> (...) sensitivity to high-level physics questions **must account for systematic uncertainties**, which involve a nonlinear trade-off between the typical machine learning performance metrics and the systematic uncertainty estimates.

This is the crux of the issue: we're not accounting for uncertainty. Our data analysis process comes with many sources of systematic error, which we endeavour to model in the likelihood function as nuisance parameters.
- e.g. simulation comes with many physics parameters that we have to choose, e.g. jes
- systs between simulators
- quantification of data/MC agreement (closure?)
...etc

But this is all talk... let's prove it!


### A simplified analysis example, both with and without uncertainty

Let's define an analysis with a predicted number of signal and background events (e.g. from simulation), with some uncertainty on the background estimate. We'll abstract the analysis configuration into a single parameter $\phi$ like so:

$$s = 15 + \phi $$
$$b = 45 - 2 \phi $$
$$\sigma_b = 0.5 + 0.1*\phi^2 $$

Note that $s \propto \phi$ and $\propto -2\phi$, so increasing $\phi$ corresponds to increasing the signal/backround ratio. However, our uncertainty scales like $\phi^2$, so we're also going to compromise in our certainty of the background count as we do that. This kind of tradeoff between $s/b$ ratio and uncertainty is important for the discovery of a new signal, so it may be that can't get away with optimizing $s/b$ alone, as the $p$-value may be worse!

Let's start by visualizing the model itself, which we do for three values of $\phi$ as an example in @fig-simple-model.

```{python}
#| label: fig-simple-model
#| fig-cap: "Plot of the predicted counts from our model at three values of $\\phi$."
import jax
import jax.numpy as jnp
import matplotlib.pyplot as plt
import optax
from jaxopt import OptaxSolver
import relaxed
from functools import partial
import matplotlib.lines as mlines
from plothelp import autogrid

subplot_settings = dict(figsize=[7, 3], dpi=150, tight_layout=True)


# model definition
def yields(phi, uncertainty=True):
    s = 15 + phi
    b = 45 - 2 * phi
    db = (
        0.5 + 0.1 * phi**2 if uncertainty else jnp.zeros_like(phi) + 0.001
    )  # small enough to be negligible
    return jnp.asarray([s]), jnp.asarray([b]), jnp.asarray([db])

# just plotting code
def yield_plot(dct):
    ax, phi, i = list(dct.values())
    s, b, db = yields(phi)
    s, b, db = s.ravel(), b.ravel(), db.ravel()  # everything is [[x]] for pyhf
    ax.set_ylim((0, 80))
    b1 = ax.bar(0.5, b, facecolor="C1", label="b")
    b2 = ax.bar(0.5, s, bottom=b, facecolor="C9", label="s")
    b3 = ax.bar(
        0.5, db, bottom=b - db / 2, facecolor="k", alpha=0.5, label=r"$\sigma_b$"
    )
    ax.set_title(r"$\phi = $" + f'{phi}')
    ax.set_xlabel("x")
    if i ==0 :
        ax.set_ylabel("yield")
    ax.set_xticks([])
    if i==2:
        ax.legend([b1, b2, b3], ["b", "s", r"$\sigma_b$"], frameon=False, bbox_to_anchor=(1.04, 0.5), loc="center left", borderaxespad=0)

autogrid(
    [0,5,10],
    yield_plot,
    subplot_kwargs={**subplot_settings, **dict(sharex=True, sharey=True)},
);

```

Using this very simple histogram, we can form a statistical model as if we're using @sec-hifa principles, which would look something like

$$
p(x | \mu) = \mathrm{Poisson}(x | \mu x^{\mathrm{sig}}  + \gamma x^{\mathrm{bkg}})~,
$$ {#eq-simplemodel}

where $\gamma$ is a continuous description of $\sigma_b$ that we get from interpolating between the yields, just like in the HistFactory approach.

Using this likelihood, we can calculate the expected discovery $p$-value by doing a hypothesis test using the observed data as the Asimov dataset for the nominal model $\mu, \gamma = 1$. We can plot this across all the values of $\phi$, and see what value gives us the lowest $p$-value (in practice, scanning over the space is computationally impossible for a given analysis configuration and a complicated model). We do this in @fig-simple-model-pval, where we include the result using a model both with and without uncertainty. Notice how much the curves differ; if we optimized the model without uncertainty (i.e. optimize for signal/background separation only), we'd end up at the *worst* solution! This is pathologically constructed of course, but it goes to show that these objectives don't talk to each other directly.

```{python}
#| label: fig-simple-model-pval
#| fig-cap: "Plot of the calculated $p$-value from using our statistical model across of $\\phi$, both including the uncertainty and neglecting it."
# our analysis pipeline, from phi to p-value
def pipeline(phi, return_yields=False, uncertainty=True):

    # grab the yields at the value of phi we want
    y = yields(phi, uncertainty=uncertainty)

    # use a dummy version of pyhf for simplicity + compatibility with jax
    model = relaxed.dummy_pyhf.uncorrelated_background(*y)

    # calculate expected discovery significance
    nominal_pars = jnp.array([1.0, 1.0])  # sets gamma, mu =1 in gamma*b + mu*s
    data = model.expected_data(nominal_pars)  # Asimov data

    # do the hypothesis test (and fit model pars with gradient descent)
    pvalue = relaxed.infer.hypotest(
        0.0,  # value of mu for the alternative hypothesis (background-only)
        data,
        model,
        test_stat="q0",  # discovery significance test
        lr=1e-3,  # learning rate for the minimization loop
        expected_pars=nominal_pars,  # optionally providing MLE pars in advance
    )
    if return_yields:
        return pvalue, y
    else:
        return pvalue
# calculate p-values for a range of phi values
phis = jnp.linspace(0, 10, 100)

# with uncertainty
pipe = partial(pipeline, return_yields=True, uncertainty=True)
pvals, ys = jax.vmap(pipe)(phis)  # map over phi grid

# without uncertainty
pipe_no_uncertainty = partial(pipeline, uncertainty=False)
pvals_no_uncertainty = jax.vmap(pipe_no_uncertainty)(phis)
fig, ax = plt.subplots()
axs = [ax]
axs[0].plot(phis, pvals, label="with uncertainty", color="C2")
axs[0].plot(phis, pvals_no_uncertainty, label="no uncertainty", color="C4")
axs[0].set_ylabel("$p$-value")
# plot vertical dotted line at minimum of p-values + s/b
best_phi = phis[jnp.argmin(pvals)]
axs[0].axvline(x=best_phi, linestyle="dotted", color="C2", label="optimal p-value")
axs[0].axvline(
    x=phis[jnp.argmin(pvals_no_uncertainty)],
    linestyle="dotted",
    color="C4",
    label=r"optimal $s/b$",
)
axs[0].legend(loc="upper left", ncol=2)
axs[0].set_xlabel("$\phi$")
plt.suptitle("Discovery p-values, with and without uncertainty")
plt.tight_layout()
```

If we optimize this analysis then, we want to arrive at the value of $\phi$ at the dotted green line (around ~4.3 or so), which gives us the benefit of rejecting the background hypothesis more strongly when the signal exists in the data. This is made possible if we use the $p$-value as our objective -- it clearly accounts for the uncertainty!

<!-- ### How do we optimize in an uncertainty-aware way?

Attempts:
- Asimov sig with assumptions on bkg uncert: [@asimovuncert]
- Learning to pivot: [@pivot]
- Directly incorporate NPs: [@uncert] -->

 The reason for this makes sense: in these physics likelihoods, we're  careful to include all the details of the systematic uncertainties that we're able to quantify by constructing nuisance parameters that vary the shape and normalization of the model. From here, to calculate the $p$-value, we then construct the **profile likelihood ratio** as a test statistic, which accounts for these systematic uncertainties by fitting the value of the nuisance parameters depending on the hypothesis you test (see @sec-hyptests for more).

All this makes the $p$-value seem like a good candidate for an objective function! So why haven't we used this already?

As emphasized in @sec-gradient-descent, if we want to perform optimization using gradient-based methods,^[We don't have to use gradient based methods! They're just very well implemented and studied, as well as enabling things like this paradigm.] then we need the objective that we optimize to be *differentiable*. This is not immediately the case for the $p$-value -- we would have to be able to differentiate through all stages of the full calculation, including model building, profiling, and even histograms, which are not generally known for their smoothness. But say we were able to decompose this complicated pipeline into bite-size chunks, each of which we can find a way to take gradients of. What becomes possible then? This begins our view of **physics analysis as a differentiable program**.

In the following sections, we'll take a collider physics analysis apart step-by-step, then see how we can employ tricks and substitutes to recover gradients for each piece. After that, we'll explore the ways that we can use the result to perform gradient-based optimization of different parts of the analysis with respect to physics goals. We'll then do it all at once by*optimizing a toy physics analysis from end-to-end*, exploring the common example of a summary statistic based on a neural network, accounting for uncertainties all the while.

## Making HEP Analysis Differentiable

The goal of this section is to study components within a HEP analysis chain that are not typically differentiable, and show that when we overcome this, we can employ the use of gradient-based optimization methods -- both to optimize free parameters jointly, and to use objectives we care about. From there, we'll examine the typical steps needed to calculate the sensitivity of a physics analysis, and see how we can make that whole chain differentiable at once, opening up a way to incorporate the full inference procedure when finding the best analysis configuration.

First, we're going to jump right in with an example to illustrate how we can take advantage of gradient descent to optimize a typical problem faced in collider physics analyses: choosing the best selection criteria.

<!-- - something about surrogates vs gradient tricks (e.g. approximations or using theoretical results)

- something about the fact that surrogates can be used *just* for optimization if the hard version of the op isn't desirable, e.g. assuming the histogram bins follow a poisson distribution (not true if you're approximating the underlying binning process) -->

### A simple example: cut optimization with gradient descent

We begin with a toy signal and background distribution over some variable $x$, where the signal lies as a peak on top of an exponentially decaying background, as shown in @fig-exp-bkg

```{python}
#| label: fig-exp-bkg
#| fig-cap: "Histogram of a situation with a simple exponentially falling background and a small signal peak."
import relaxed
from functools import partial
from jax.random import PRNGKey
# generate background data from an exponential distribution with a little noise
def generate_background(key, n_samples, n_features, noise_std):
    key, subkey = jax.random.split(key, 2)
    data = jax.random.exponential(subkey, (n_samples, n_features))
    key, subkey = jax.random.split(key, 2)
    data += jax.random.normal(subkey, (n_samples, n_features)) * noise_std
    return data


# generate signal data from a normal distribution close to the background
def generate_signal(key, n_samples, n_features):
    key, subkey = jax.random.split(key, 2)
    data = jax.random.normal(subkey, (n_samples, n_features)) / 2 + 2
    return data


# get 1000 samples from the background and 100 samples from the signal
bkg = generate_background(PRNGKey(0), 1000, 1, 0.1).ravel()
sig = generate_signal(PRNGKey(1), 100, 1).ravel()

sig = sig[sig>0]
bkg = bkg[bkg>0]

fig, ax = plt.subplots(**subplot_settings)
# plot!
ax.hist(
    [bkg, sig], stacked=True, bins=30, histtype="step", label=["background", "signal"]
)
ax.set_xlabel("x")
ax.set_ylabel("count")
ax.legend();
```

A quintessential operation for data filtering in HEP is the simple threshold, also called a **cut**: we keep all data above (or below) a certain value of the quantity we're concerned with. To increase the significance (e.g. as defined by @eq-asimov-significance), we can try to remove data such that we increase the overall ratio of signal to background. In @fig-exp-bkg, it looks like there's not much signal for low values of $x$, which motivates us to put a cut at say $x=1$. We can see the result of applying this cut in @fig-compare-cut, where we've increased the Asimov significance compared to using no cut at all.

```{python}
#| label: fig-compare-cut
#| fig-cap: "Comparing the significance resulting from applying a cut to no cut at all."
def significance_after_cut(cut):
    # treat analysis as a one-bin counting experiment
    s = len(sig[sig > cut]) + 1e-1
    b = len(bkg[bkg > cut]) + 1e-1
    return relaxed.metrics.asimov_sig(s, b)  # stat-only significance


cut = 1  # change me to change the plot!

def make_cut_plot(cut, ax):
    significance = significance_after_cut(cut)
    ax.hist(
        [bkg, sig], stacked=True, bins=30, histtype="step", label=["background", "signal"]
    )
    ax.axvline(x=cut, color="k", linestyle="--", alpha=0.5, label=f"cut = {cut:.2f}")
    ax.axvspan(0,cut, hatch='//', color="grey", alpha=0.3,zorder=-999)
    ax.text(
        0.7,
        0.2,
        f"significance = {significance:.2f}",
        ha="center",
        va="center",
        transform=plt.gca().transAxes,
    )
    ax.set_xlabel("x")
    ax.legend()

fig, axs = plt.subplots(1,2,**subplot_settings, sharey=True)
ax = axs[0]
# plot!
ax.hist(
    [bkg, sig], stacked=True, bins=30, histtype="step", label=["background", "signal"]
)
ax.set_xlabel("x")
ax.set_ylabel("count")
significance = significance_after_cut(0)
ax.text(
    0.7,
    0.2,
    f"significance = {significance:.2f}",
    ha="center",
    va="center",
    transform=plt.gca().transAxes,
)
ax.legend()
make_cut_plot(cut, axs[1])
```

We had a nice go at a guess, but how do we pick the *best* cut? For this simple problem, it suffices to scan over the different significances we'll get by cutting at each value of $x$, then just use the value with the highest significance. We do this in @fig-cut-scan, where the optimal cut seems to be around $x=1.54$.

```{python}
#| label: fig-cut-scan
#| fig-cap: "A scan over all cut values to find the best resulting Asimov significance."
# plot significance for all cut values
cut_values = jnp.linspace(0, 8, 100)
significances_hard = jnp.array([significance_after_cut(cut) for cut in cut_values])

fig, ax = plt.subplots(**subplot_settings)
ax.plot(cut_values, significances_hard, label="significance")
optimal_cut = cut_values[jnp.argmax(significances_hard)]
ax.axvline(x=optimal_cut, color="k", linestyle="--", alpha=0.5, label="optimal cut")
ax.text(
    0.7,
    0.5,
    f"optimal cut = {optimal_cut:.2f}",
    ha="center",
    va="center",
    transform=plt.gca().transAxes,
)
ax.set_xlabel("x")
ax.set_ylabel("$Z_A$")
```

In reality, this could be an expensive procedure to do for a wide range of $x$ and for many different cut variables. This prompts the search for some kind of intelligent optimization that can handle large dimensional parameter spaces. Gradient descent is just that! But, to make it work, we need to be able to calculate the gradient of the significance with respect to the cut value -- something only possible if the cut itself is differentiable (it isn't).

To see this, note that cuts are step functions, i.e. logical less than or more than statements. These can be viewed as applying weights to the data -- 0 on one side of the threshold, and 1 on the other. If we change the cut value, the events either keep their weight (0 change in significance) or sharply gain/lose their weight value (discrete jump in significance). We would then like to replace this thresholding with a *smooth* weight assignment such that the cut value varies smoothly with the weights applied. What kind of operation can do this? We have such a candidate in the *sigmoid function* $1/(1+e^{-x})$.

Normally, the sigmoid serves as a method to map values on the real line to [0,1], so we leverage this to be used as a cut by applying it to data, which results in a set of weights for each point in [0,1]. (A normal cut does this too, but the weights are all 0 or 1, and you drop the 0s. One could similarly threshold on a minimum weight value here.)

Practically, we introduce slope and intercept terms that control the sigmoid's x position and how "hard" the cut is: $1/(1+e^{-\mathrm{slope}(x-\mathrm{cut~value}})$. High slopes mean less approximate cuts, but at the risk of gradient instability.

--plot

- a differentiable version of this admits real numbers in-between these values, smoothing out the step
- example: sigmoid
- you can tune the steepness of the slope by adding a number
--plot

### Examining a typical analysis

Given a pre-filtered dataset, a commonly used analysis pipeline in HEP involves the
following stages:


1.  Construction of a learnable 1-D summary statistic from data (with
    parameters $\phi$)

2.  Binning of the summary statistic, e.g. through a histogram

3.  Statistical model building, using the summary statistic as a
    template

4.  Calculation of a test statistic, used to perform a frequentist
    hypothesis test of signal versus background

5.  A $p$-value (or $\mathrm{CL_s}$ value) resulting from that
    hypothesis test, used to characterize the sensitivity of the
    analysis

We can express this workflow as a direct function of the input dataset
$\mathcal{D}$ and observable parameters $\phi$:

$$
    \mathrm{CL_s} = f(\mathcal{D},\phi) = (f_{\mathrm{sensitivity}} \circ f_{\mathrm{test\,stat}} \circ f_{\mathrm{likelihood}}  \circ f_{\mathrm{histogram}}  \circ f_{\mathrm{observable}})(\mathcal{D},\phi).
$$ {#eq-neos}

Is this going to be differentiable? To calculate $\partial \text{CL}_s / \partial \varphi$, we'll have to split this up by the chain rule into the different components:

$$

$${#eq-analysis-chain-rule}

In the case of an observable defined gradients with respect to $\phi$ (e.g. a neural network), the



### Binned density estimation (histograms)

Histograms are discontinuous by nature. They are defined for 1-D data as a set of two quantities: intervals (or *bins*) over the domain of that data, and counts of the number of data points that fall into each bin. For small changes in the underlying data distribution, bin counts will either remain static, or jump in integer intervals as data migrate between bins, both of which result in ill-defined gradients. To demonstrate this, let's examine what happens to a histogram when we shift the underlying distribution by a constant factor.







We address
this inherent non-differentiability through implementing a
differentiable surrogate: a histogram based on a *kernel density
estimate* (KDE).

A KDE is a "non-parametric\" density estimate based on defining a kernel
function $K$ centred on each data point. Then, the density at an
evaluation point $x$ is the average of the contributions of each kernel
function at that point.

a with the full density given as $p(t) = 1/n\sum_i K(t,t_i)$. Normally,
a popular kernel function choice is the standard normal distribution,
which comes with a parameter called the **bandwidth** that affects the
smoothness of the resulting density estimate.

Coming back to gradients: in our case, the data $t_i$ we construct the
density estimate over are themselves functions of the summary statistic
parameters, i.e. $t_i = f(x_i;\phi)$. The resulting density estimate
$p(t|\phi)$ will then be differentiable as long as the kernel $K$ is
differentiable with respect to $t_i$, and by extension with respect to
$\phi$. To extend this differentiability in a binned fashion, we can
accumulate the probability mass of the KDE within the bin edges of the
original histogram -- equivalent to evaluations of the Gaussian
cumulative density function -- to convert $p(t|\phi)$ to a **binned KDE
(bKDE)**, i.e. a set of discrete per-bin probabilities $p_i(\phi)$.
