---
execute:
  echo: false
format:
  html:
    default-image-extension: svg
  pdf:
    default-image-extension: pdf
jupyter: python3
---

# Physics Analysis as a Differentiable Program

## Motivation
Given the success of the Standard Model (SM), analysis of data from the LHC usually occurs for two reasons:
- Precisely measuring SM processes to look for small deviations with SM predictions
- Searching for new physics signatures as predicted by models beyond the SM

When analysing data in this way, we'll have lots of free parameters to tune. These can be as simple as a threshold value that you limit the p_T to, or as complicated as the weights and biases that determine a neural network for identifying $b$-jets. We can of course choose any values for these quantities to do our analysis, but the resulting physics that follows may suffer as a result. As such, we're likely to try some kind of optimization to improve the answers to our physics questions. How do we do this in practice?

In either case above, there is a notion of <span style="color:#13becf">signal</span> (what you’re looking for) and <span style="color:#ff7f0e">background</span> (everything else).
Generally, we then try to choose a parameter configuration that can separate (or discriminate) the signal from the background, allowing us to extract just the data we think is relevant to the physics process we're looking at. As an example, machine learning models are often trained using the **binary cross-entropy** loss as an objective, which corresponds to optimizing the ability of the model to identify whether an event originated from signal or background processes. A closely related goal is the **Asimov significance** in the case of signal and background event counts $s$ and $b$ with *no uncertainty* on either quantity. The formula for this stems from assuming a Poisson likelihood function, and is equal to

$$
Z_A = \sqrt{2\sum_{i\in bins}((s_i + b_i)(\log{(1 + s_i / b_i)}) - s_i)}~.
$$ {#eq-asimov-significance}

As indicated in the sum, these counts can be spread across different bins in the case where your data is a histogram, but the formula is more commonly reduced to the 1-bin scenario that just deals with the overall numbers of signal and background events. In this case, we can then Taylor expand the logarithm to get

$$Z_A = \sqrt{2((s+b)(s/b + \mathcal{O}(s/b) - s)} \approx s/\sqrt{b}~~~\mathrm{for}~s<<b.$$

This makes it much clearer to see that optimising with respect to $Z_A$ is just a fancier way of trying to increase the amount of signal compared to the amount of background, which is directly analogous to separating signal from background, just as binary cross-entropy would do.

Now, this is all very sensible of course (we want to discover our signal), but this approach has some shortcomings that distance the efficacy of the resulting configuration from our physics goals. A recent review of deep learning in LHC physics [@deeplhc] lets us in on why:

> (...) tools are often optimized for performance on a particular task that is **several steps removed from the ultimate physical goal** of searching for a new particle or testing a new physical theory.

> (...) sensitivity to high-level physics questions **must account for systematic uncertainties**, which involve a nonlinear trade-off between the typical machine learning performance metrics and the systematic uncertainty estimates.

This is the crux of the issue: we're not accounting for uncertainty. Our data analysis process comes with many sources of systematic error, which we endeavour to model in the likelihood function as nuisance parameters.
- e.g. simulation comes with many physics parameters that we have to choose, e.g. jes
- systs between simulators
- quantification of data/MC agreement (closure?)
...etc

But this is all talk... let's prove it!


### A simplified analysis example, both with and without uncertainty {#sec-simple-anal}

Let's define an analysis with a predicted number of signal and background events (e.g. from simulation), with some uncertainty on the background estimate. We'll abstract the analysis configuration into a single parameter $\phi$ like so:

$$s = 15 + \phi $$
$$b = 45 - 2 \phi $$
$$\sigma_b = 0.5 + 0.1*\phi^2 $$

Note that $s \propto \phi$ and $\propto -2\phi$, so increasing $\phi$ corresponds to increasing the signal/backround ratio. However, our uncertainty scales like $\phi^2$, so we're also going to compromise in our certainty of the background count as we do that. This kind of tradeoff between $s/b$ ratio and uncertainty is important for the discovery of a new signal, so it may be that can't get away with optimizing $s/b$ alone, as the $p$-value may be worse!

Let's start by visualizing the model itself, which we do for three values of $\phi$ as an example in @fig-simple-model.

```{python}
#| label: fig-simple-model
#| fig-cap: "Plot of the predicted counts from our model at three values of $\\phi$."
import jax
import jax.numpy as jnp
import matplotlib.pyplot as plt
import optax
from jaxopt import OptaxSolver
import relaxed
from functools import partial
import matplotlib.lines as mlines
from plothelp import autogrid

subplot_settings = dict(figsize=[7, 3], dpi=150, tight_layout=True)


# model definition
def yields(phi, uncertainty=True):
    s = 15 + phi
    b = 45 - 2 * phi
    db = (
        0.5 + 0.1 * phi**2 if uncertainty else jnp.zeros_like(phi) + 0.001
    )  # small enough to be negligible
    return jnp.asarray([s]), jnp.asarray([b]), jnp.asarray([db])

# just plotting code
def yield_plot(dct):
    ax, phi, i = list(dct.values())
    s, b, db = yields(phi)
    s, b, db = s.ravel(), b.ravel(), db.ravel()  # everything is [[x]] for pyhf
    ax.set_ylim((0, 80))
    b1 = ax.bar(0.5, b, facecolor="C1", label="b")
    b2 = ax.bar(0.5, s, bottom=b, facecolor="C9", label="s")
    b3 = ax.bar(
        0.5, db, bottom=b - db / 2, facecolor="k", alpha=0.5, label=r"$\sigma_b$"
    )
    ax.set_title(r"$\phi = $" + f'{phi}')
    ax.set_xlabel("x")
    if i ==0 :
        ax.set_ylabel("yield")
    ax.set_xticks([])
    if i==2:
        ax.legend([b1, b2, b3], ["b", "s", r"$\sigma_b$"], frameon=False, bbox_to_anchor=(1.04, 0.5), loc="center left", borderaxespad=0)

autogrid(
    [0,5,10],
    yield_plot,
    subplot_kwargs={**subplot_settings, **dict(sharex=True, sharey=True)},
);

```

Using this very simple histogram, we can form a statistical model as if we're using @sec-hifa principles, which would look something like

$$
p(x | \mu) = \mathrm{Poisson}(x | \mu x^{\mathrm{sig}}  + \gamma x^{\mathrm{bkg}})~,
$$ {#eq-simplemodel}

where $\gamma$ is a continuous description of $\sigma_b$ that we get from interpolating between the yields, just like in the HistFactory approach.

Using this likelihood, we can calculate the expected discovery $p$-value by doing a hypothesis test using the observed data as the Asimov dataset for the nominal model $\mu, \gamma = 1$. We can plot this across all the values of $\phi$, and see what value gives us the lowest $p$-value (in practice, scanning over the space is computationally impossible for a given analysis configuration and a complicated model). We do this in @fig-simple-model-pval, where we include the result using a model both with and without uncertainty. Notice how much the curves differ; if we optimized the model without uncertainty (i.e. optimize for signal/background separation only), we'd end up at the *worst* solution! This is pathologically constructed of course, but it goes to show that these objectives don't talk to each other directly.

```{python}
#| label: fig-simple-model-pval
#| fig-cap: "Plot of the calculated $p$-value from using our statistical model across of $\\phi$, both including the uncertainty and neglecting it."
# our analysis pipeline, from phi to p-value
def pipeline(phi, return_yields=False, uncertainty=True):

    # grab the yields at the value of phi we want
    y = yields(phi, uncertainty=uncertainty)

    # use a dummy version of pyhf for simplicity + compatibility with jax
    model = relaxed.dummy_pyhf.uncorrelated_background(*y)

    # calculate expected discovery significance
    nominal_pars = jnp.array([1.0, 1.0])  # sets gamma, mu =1 in gamma*b + mu*s
    data = model.expected_data(nominal_pars)  # Asimov data

    # do the hypothesis test (and fit model pars with gradient descent)
    pvalue = relaxed.infer.hypotest(
        0.0,  # value of mu for the alternative hypothesis (background-only)
        data,
        model,
        test_stat="q0",  # discovery significance test
        lr=1e-3,  # learning rate for the minimization loop
        expected_pars=nominal_pars,  # optionally providing MLE pars in advance
    )
    if return_yields:
        return pvalue, y
    else:
        return pvalue
# calculate p-values for a range of phi values
phis = jnp.linspace(0, 10, 100)

# with uncertainty
pipe = partial(pipeline, return_yields=True, uncertainty=True)
pvals, ys = jax.vmap(pipe)(phis)  # map over phi grid

# without uncertainty
pipe_no_uncertainty = partial(pipeline, uncertainty=False)
pvals_no_uncertainty = jax.vmap(pipe_no_uncertainty)(phis)
fig, ax = plt.subplots(**subplot_settings)
axs = [ax]
axs[0].plot(phis, pvals, label="with uncertainty", color="C2")
axs[0].plot(phis, pvals_no_uncertainty, label="no uncertainty", color="C4")
axs[0].set_ylabel("$p$-value")
# plot vertical dotted line at minimum of p-values + s/b
best_phi = phis[jnp.argmin(pvals)]
axs[0].axvline(x=best_phi, linestyle="dotted", color="C2", label="optimal p-value")
axs[0].axvline(
    x=phis[jnp.argmin(pvals_no_uncertainty)],
    linestyle="dotted",
    color="C4",
    label=r"optimal $s/b$",
)
axs[0].legend(loc="upper left", ncol=2)
axs[0].set_xlabel("$\phi$")
plt.suptitle("Discovery p-values, with and without uncertainty")
plt.tight_layout()
```

If we optimize this analysis then, we want to arrive at the value of $\phi$ at the dotted green line (around ~4.3 or so), which gives us the benefit of rejecting the background hypothesis more strongly when the signal exists in the data. This is made possible if we use the $p$-value as our objective -- it clearly accounts for the uncertainty!

<!-- ### How do we optimize in an uncertainty-aware way?

Attempts:
- Asimov sig with assumptions on bkg uncert: [@asimovuncert]
- Learning to pivot: [@pivot]
- Directly incorporate NPs: [@uncert] -->

 The reason for this makes sense: in these physics likelihoods, we're  careful to include all the details of the systematic uncertainties that we're able to quantify by constructing nuisance parameters that vary the shape and normalization of the model. From here, to calculate the $p$-value, we then construct the **profile likelihood ratio** as a test statistic, which accounts for these systematic uncertainties by fitting the value of the nuisance parameters depending on the hypothesis you test (see @sec-hyptests for more).

All this makes the $p$-value seem like a good candidate for an objective function! So why haven't we used this already?

As emphasized in @sec-gradient-descent, if we want to perform optimization using gradient-based methods,^[We don't have to use gradient based methods! They're just very well implemented and studied, as well as enabling things like this paradigm.] then we need the objective that we optimize to be *differentiable*. This is not immediately the case for the $p$-value -- we would have to be able to differentiate through all stages of the full calculation, including model building, profiling, and even histograms, which are not generally known for their smoothness. But say we were able to decompose this complicated pipeline into bite-size chunks, each of which we can find a way to take gradients of. What becomes possible then? This begins our view of **physics analysis as a differentiable program**.

In the following sections, we'll take a collider physics analysis apart step-by-step, then see how we can employ tricks and substitutes to recover gradients for each piece. After that, we'll explore the ways that we can use the result to perform gradient-based optimization of different parts of the analysis with respect to physics goals. We'll then do it all at once by*optimizing a toy physics analysis from end-to-end*, exploring the common example of a summary statistic based on a neural network, accounting for uncertainties all the while.

## Making HEP Analysis Differentiable

The goal of this section is to study components within a HEP analysis chain that are not typically differentiable, and show that when we overcome this, we can employ the use of gradient-based optimization methods -- both to optimize free parameters jointly, and to use objectives we care about. From there, we'll examine the typical steps needed to calculate the sensitivity of a physics analysis, and see how we can make that whole chain differentiable at once, opening up a way to incorporate the full inference procedure when finding the best analysis configuration.

First, we're going to jump right in with an example to illustrate how we can take advantage of gradient descent to optimize a typical problem faced in collider physics analyses: choosing the best selection criteria.


### A simple example: cut optimization with gradient descent

We begin with a toy signal and background distribution over some variable $x$, where the signal lies as a peak on top of an exponentially decaying background, as shown in @fig-exp-bkg.

```{python}
#| label: fig-exp-bkg
#| fig-cap: "Histogram of a situation with a simple exponentially falling background and a small signal peak."
import relaxed
from functools import partial
from jax.random import PRNGKey
# generate background data from an exponential distribution with a little noise
def generate_background(key, n_samples, n_features, noise_std):
    key, subkey = jax.random.split(key, 2)
    data = jax.random.exponential(subkey, (n_samples, n_features))
    key, subkey = jax.random.split(key, 2)
    data += jax.random.normal(subkey, (n_samples, n_features)) * noise_std
    return data


# generate signal data from a normal distribution close to the background
def generate_signal(key, n_samples, n_features):
    key, subkey = jax.random.split(key, 2)
    data = jax.random.normal(subkey, (n_samples, n_features)) / 2 + 2
    return data


# get 1000 samples from the background and 100 samples from the signal
bkg = generate_background(PRNGKey(0), 1000, 1, 0.1).ravel()
sig = generate_signal(PRNGKey(1), 100, 1).ravel()

sig = sig[sig>0]
bkg = bkg[bkg>0]

fig, ax = plt.subplots(**subplot_settings)
# plot!
ax.hist(
    [bkg, sig], stacked=True, bins=30, histtype="step", label=["background", "signal"]
)
ax.set_xlabel("x")
ax.set_ylabel("count")
ax.legend();
```

A quintessential operation for data filtering in HEP is the simple threshold, also called a **cut**: we keep all data above (or below) a certain value of the quantity we're concerned with. To increase the significance (e.g. as defined by @eq-asimov-significance), we can try to remove data such that we increase the overall ratio of signal to background. In @fig-exp-bkg, it looks like there's not much signal for low values of $x$, which motivates us to put a cut at say $x=1$. We can see the result of applying this cut in @fig-compare-cut, where we've increased the Asimov significance compared to using no cut at all.

```{python}
#| label: fig-compare-cut
#| fig-cap: "Comparing the significance resulting from applying a cut to no cut at all."

def significance_after_cut(cut):
    # treat analysis as a one-bin counting experiment
    s = len(sig[sig > cut]) + 1e-1
    b = len(bkg[bkg > cut]) + 1e-1
    return relaxed.metrics.asimov_sig(s, b)  # stat-only significance


cut = 1  # change me to change the plot!

def make_cut_plot(cut, ax):
    significance = significance_after_cut(cut)
    ax.hist(
        [bkg, sig], stacked=True, bins=30, histtype="step", label=["background", "signal"]
    )
    ax.axvline(x=cut, color="k", linestyle="--", alpha=0.5, label=f"cut = {cut:.2f}")
    ax.axvspan(0,cut, hatch='//', color="grey", alpha=0.3,zorder=-999)
    ax.text(
        0.7,
        0.2,
        f"significance = {significance:.2f}",
        ha="center",
        va="center",
        transform=ax.transAxes,
    )
    ax.set_xlabel("x")
    ax.legend()

fig, axs = plt.subplots(1,2,**subplot_settings, sharey=True)
ax = axs[0]
# plot!
ax.hist(
    [bkg, sig], stacked=True, bins=30, histtype="step", label=["background", "signal"]
)
ax.set_xlabel("x")
ax.set_ylabel("count")
significance = significance_after_cut(0)
ax.text(
    0.7,
    0.2,
    f"significance = {significance:.2f}",
    ha="center",
    va="center",
    transform=ax.transAxes,
)
ax.legend()
make_cut_plot(cut, axs[1])
```

We had a nice go at a guess, but how do we pick the *best* cut? For this simple problem, it suffices to scan over the different significances we'll get by cutting at each value of $x$, then just use the value with the highest significance. We do this in @fig-cut-scan, where the optimal cut seems to be around $x=1.54$.
<!--
```{python}
#| label: fig-cut-scan
#| fig-cap: "A scan over all cut values to find the best resulting Asimov significance."

cut_values = jnp.linspace(0, 8, 100)
significances_hard = jnp.array([significance_after_cut(cut) for cut in cut_values])

fig, ax = plt.subplots(**subplot_settings)
ax.plot(cut_values, significances_hard, label="significance")
optimal_cut = cut_values[jnp.argmax(significances_hard)]
ax.axvline(x=optimal_cut, color="k", linestyle="--", alpha=0.5, label="optimal cut")
ax.text(
    0.7,
    0.5,
    f"optimal cut = {optimal_cut:.2f}",
    ha="center",
    va="center",
    transform=plt.gca().transAxes,
)
ax.set_xlabel("x")
ax.set_ylabel(r"$Z_A$")
```
-->
In reality, this could be an expensive procedure to do for a wide range of $x$ and for many different cut variables. This prompts the search for some kind of intelligent optimization that can handle large dimensional parameter spaces. Gradient descent is just that! But, to make it work, we need to be able to calculate the gradient of the significance with respect to the cut value -- something only possible if the cut itself is differentiable (it isn't).

To see this, note that cuts are step functions, i.e. logical less than or more than statements. These can be viewed as applying weights to the data -- 0 on one side of the threshold, and 1 on the other. If we change the cut value, the events either keep their weight (0 change in significance) or sharply gain/lose their weight value (discrete jump in significance). We would then like to replace this thresholding with a *smooth* weight assignment such that the cut value varies smoothly with the weights applied. What kind of operation can do this? We have such a candidate in the *sigmoid function* $1/(1+e^{-x})$.

Normally, the sigmoid serves as a method to map values on the real line to [0,1], so we leverage this to be used as a cut by applying it to data, which results in a set of weights for each point in [0,1]. (A normal cut does this too, but the weights are all 0 or 1, and you drop the 0s. One could similarly threshold on a minimum weight value here.)

Practically, we introduce slope and intercept terms that control the sigmoid's $x$ position and how "hard" the cut is: $1/(1+e^{-\mathrm{slope}(x-\mathrm{cut~value}})$. This slope allows us to control the degree to which we approximate the cut as a thresholding operation, with higher values of the slope meaning less approximation (but this will also increase the variance of the gradients, as we're getting closer to the discrete situation outlined previously). See the sigmoid plotted with different slopes in @fig-sigmoid.



```{python}
#| label: fig-sigmoid
#| fig-cap: "Comparing the sigmoid to a regular hard cut for different values of the sigmoid slope."

# plot significance for all cut values
cut_val = 5  # translates on the x-axis
fig, ax = plt.subplots(**subplot_settings)
ax.plot(cut_values, cut_values > cut_val, label="hard cut")
ax.plot(
    cut_values, relaxed.cut(cut_values, cut_val, slope=1), label="sigmoid (slope=1)", color="C1"
)
ax.plot(
    cut_values,
    relaxed.cut(cut_values, cut_val, slope=10),
    label="sigmoid (slope=10)",
    color="C2",
    alpha=0.4,
)
ax.plot(
    cut_values,
    relaxed.cut(cut_values, cut_val, slope=0.5),
    label="sigmoid (slope=0.5)",
    color="C3",
    alpha=0.4,
)
ax.set_ylabel("weight applied at x")
ax.set_xlabel("x")
ax.legend();
```

Now that we have a differentiable cut, we can see what the significance scan from @fig-cut-scan looks like for the differentiable case, shown in @fig-cut-scan-2. It's an interesting plot; there's a clear smoothing out of the overall envelope of the significance in comparison to using the hard cut. However, the important thing is the **coincidence of the maxima**: when optimizing, we'll use the differentiable cut, but we'll plug the value of the cut position from the optimization back in to the hard cut for our actual physics results. This is a very important distinction - *we don't use approximate operations in the final calculation!* Moreover, since we can control the degree to which we're approximating the significance landscape, one could even imagine a fine-tuning of the slope when we're close to a local minima during optimization, allowing us to make jumps more in-line with the true optimum value (though this is not explored here).

```{python}
#| label: fig-cut-scan-2
#| fig-cap: "A scan over all cut values to find the best resulting Asimov significance -- both for the regular cut, and for the sigmoid."

# plot significance for all cut values
def significance_after_soft_cut(cut, slope):
    s_weights = (
        relaxed.cut(sig, cut, slope) + 1e-4
    )  # add small offset to avoid 0 weights
    b_weights = relaxed.cut(bkg, cut, slope) + 1e-4
    return relaxed.metrics.asimov_sig(s_weights.sum(), b_weights.sum())


# choosing the cut slope: increasing slope reduces bias but also noises gradients.
# I increased it until gradients were nan in the next step, then went a touch lower.
# I'll think about a more principled way to do this (suggestions welcome!)
slope = 2.7

fig, ax = plt.subplots(**subplot_settings)

# plot significance for all cut values
cut_values = jnp.linspace(0, 8, 100)
soft = partial(significance_after_soft_cut, slope=slope)
significances = jax.vmap(soft, in_axes=(0))(cut_values)
ax.plot(cut_values, significances_hard, label="hard cut")
ax.plot(cut_values, significances, label="sigmoid (slope=2.7)")
ax.set_xlabel("cut value")
ax.set_ylabel("$Z_A$")
ax.legend();
```

Now that we've done the groundwork, we can do the optimization and see if we converge to the correct result! Using gradient descent and the Adam optimizer with a learning rate of 1e-3, we find the cut shown in @fig-optimized-cut (we optimize $1/Z_A$ since we're doing minimization). The significance (calculated with the *hard* cut) is extremely close to the best possible value, so I'd call this a success!


```{python}
#| label: fig-optimized-cut
#| fig-cap: "The resulting cut from optimization compared to the true best cut. Significances in both cases are shown."
from jaxopt import OptaxSolver
from optax import adam

# define something to minimise (1/significance)
def loss(cut):
    s_weights = relaxed.cut(sig, cut, slope) + 1e-4
    b_weights = relaxed.cut(bkg, cut, slope) + 1e-4
    return 1 / relaxed.metrics.asimov_sig(s_weights.sum(), b_weights.sum())


fig, ax = plt.subplots(**subplot_settings)
# play with the keyword arguments to the optimiser if you want :)
solver = OptaxSolver(loss, adam(learning_rate=1e-2), maxiter=10000, tol=1e-6)
init = 6.0
cut_opt = solver.run(init).params
significance = significance_after_cut(cut_opt)
ax.hist(
    [bkg, sig], stacked=True, bins=30, histtype="step", label=["background", "signal"]
)
ax.axvline(
    x=cut_opt,
    color="r",
    linestyle="-",
    alpha=0.5,
    label=f"optimised cut = {cut_opt:.2f}",
)
significance = significance_after_cut(cut_opt)
ax.axvline(
    x=optimal_cut,
    color="k",
    linestyle="--",
    alpha=0.5,
    label=f"true best cut = {optimal_cut:.2f}",
)
ax.text(
    0.65,
    0.3,
    f"significance at optimised cut = {significance:.2f}",
    ha="center",
    va="center",
    transform=plt.gca().transAxes,
)
ax.text(
    0.65,
    0.15,
    f"significance at best cut = {significance_after_cut(optimal_cut):.2f}",
    ha="center",
    va="center",
    transform=plt.gca().transAxes,
)
ax.set_xlabel("x")
ax.set_ylabel("count")
ax.legend();
```

### Examining a typical analysis

Now that we've looked at an example of the kind of thing we may want to do, we can zoom out and look at the big picture. Given a pre-filtered dataset, a commonly used analysis pipeline in HEP involves the
following stages:


1.  Construction of a learnable 1-D summary statistic from data (with
    parameters $\varphi$)

2.  Binning of the summary statistic, e.g. through a histogram

3.  Statistical model building, using the summary statistic as a
    template

4.  Calculation of a test statistic, used to perform a frequentist
    hypothesis test of signal versus background

5.  A $p$-value (or $\mathrm{CL_s}$ value) resulting from that
    hypothesis test, used to characterize the sensitivity of the
    analysis

We can express this workflow as a direct function of the input dataset
$\mathcal{D}$ and observable parameters $\varphi$:

$$
    \mathrm{CL}_s = f(\mathcal{D},\varphi) = (f_{\mathrm{sensitivity}} \circ f_{\mathrm{test\,stat}} \circ f_{\mathrm{likelihood}}  \circ f_{\mathrm{histogram}}  \circ f_{\mathrm{observable}})(\mathcal{D},\varphi).
$$ {#eq-neos}

Is this going to be differentiable? To calculate $\partial \text{CL}_s / \partial \varphi$, we'll have to split this up by the chain rule into the different components, which can be written verbosely as

$$
\frac{\partial\,\mathrm{CL}_s}{\partial \varphi} = \frac{\partial f_{\mathrm{sensitivity}}}{\partial f_{\mathrm{test\,stat}}}\frac{\partial f_{\mathrm{test\,stat}}}{\partial f_{ \mathrm{likelihood}}} \frac{\partial f_{\mathrm{likelihood}}}{\partial f_{\mathrm{histogram}}}   \frac{\partial f_{\mathrm{histogram}}}{\partial f_{\mathrm{observable}}}  \frac{\partial f_{\mathrm{observable}}}{\partial \varphi}~.
$${#eq-analysis-chain-rule}

In the case of an observable that has well-defined gradients with respect to $\phi$ (e.g. a neural network), the last term in @eq-analysis-chain-rule is possible to calculate through automatic differentiation. But none of the other terms are differentiable by default! We're going to have to figure out some way to either *relax* (make differentiable) these operations, or use tricks to make the gradient easier to calculate. This is explored in the following sections, starting with the histogram.



### Binned density estimation (histograms) {#sec-bkde}

Histograms are discontinuous by nature. They are defined for 1-D data as a set of two quantities: intervals (or *bins*) over the domain of that data, and counts of the number of data points that fall into each bin. For small changes in the underlying data distribution, bin counts will either remain static, or jump in integer intervals as data migrate between bins, both of which result in ill-defined gradients. Similarly to the cut example with the sigmoid, we're assigning a number (there the weight, and here a count in a bin) in a discrete way to the data -- to make this differentiable, we need to come up with a smooth version of this.

The solution that we developed to this involves a **kernel density estimate** (KDE). We discussed this in @sec-kde, but just to recap: a KDE is essentially the average of a set of normal distributions centered at each data point, with their width controlled by a global parameter called the **bandwidth**. There's a neat way to take this and cast it into a bin-like form (i.e. defined over intervals): We can calculate the "count" in an interval by taking the area under the KDE between the interval endpoints. We can do this using the cumulative density function (cdf), as $P(a \leqslant X \leqslant b) = P(X \leqslant b) - P(X \leqslant a)$. Since the KDE is the mean over some normal distributions, its cdf is also just the mean of the cdfs for each normal distribution. Moreover, to turn this into a histogram-like object, we can multiply the result by the total number of events, which just changes the mean into a sum. We put this all together in @fig-bkde-code, where a pseudocoded implementation of a **binned KDE** (bKDE) can be found.

```{python}
#| label: fig-bkde-code
#| echo: true
#| eval: false
#| fig-cap: "Implementation of a bKDE using Pythonic pseudocode."
def bKDE(data: Array, bins: Array, bandwidth: float) -> Array:
    edge_hi = bins[1:]  # ending bin edges ||<-
    edge_lo = bins[:-1]  # starting bin edges ->||
    # get cumulative counts (area under kde) for each set of bin edges
    cdf_hi = norm.cdf(edge_hi.reshape(-1, 1), loc=data, scale=bandwidth)
    cdf_lo = norm.cdf(edge_lo.reshape(-1, 1), loc=data, scale=bandwidth)
    return (cdf_hi - cdf_lo).sum(axis=1) # sum cdfs over each kernel
```

We know what happens to a KDE when we change the bandwidth, but how does that impact the bKDE? We can quantify this relative to the bin width by examining the shape of the bKDE relative to a "hard" histogram, which is shown in @fig-bkde-bandwidth. For low bandwidths, we recover something almost exactly resembling a regular histogram. In fact, in the limit of zero bandwidth, we will *exactly* get a histogram! The reason is that zero bandwidth would turn each normal distribution into an infinite spike at each data point, which, when integrated over to get the counts, would have a contribution of 1 if the event lies in the bin, and 0 otherwise.

![Illustration of the bias/smoothness tradeoff when tuning the bandwidth of a bKDE, defined over 200 samples from a bi-modal Gaussian mixture. All distributions are normalized to unit area. The individual kernels that make up the KDE are scaled down for visibility.](images/relaxed_hist){#fig-bkde-bandwidth}

#### Bandwidth vs number of data points {-}



### Differentiable likelihood construction

Now, I must confess, I have told you somewhat of a white lie to set up the motivation here. The likelihood function as described in @eq-hifabase is indeed a-priori differentiable with respect to the histograms that make up the expectations. The problem is actually a *technical* one -- we need to make this happen in code. As per our conversations on automatic differentiation, we know how to do this: we code up our program using a framework for automatic differentiation that has defined primitives and gradient rules. `pyhf` (@pyhf, @pyhf2) is the software package that brings this to life: the whole HistFactory prescription, all coded using a choice of autodiff backends, e.g. JAX, TensorFlow, and PyTorch. There's honestly not too much else to say here; any further discussion would involve extremely niche and technical topics within the `pyhf` codebase, and all the error messages I saw over the years I worked on trying to hack things together. I'll spare you that discussion (feel free to ask about it), and we'll move on to something a little more thesis-suited (though, what is a PhD if not niche and technical...).

### Differentiable test statistics (profile likelihood ratio)

Recall from @sec-asymptotics that when we're constructing test statistics, we're using the building block of the *profile likelihood ratio*, which we state once again as

$$
\lambda(x, \mu) = \frac{p\left(x|\mu,\hat{\hat{\theta}}(\mu)\right)}{p\left(x| \hat{\mu}, \hat{\theta}\right)}~.
$$

The variables $\hat{\hat{\theta}}(\mu)$ and $\hat{\mu}, \hat{\theta}$ are the result of two separate maximum likelihood fits. Are these differentiable? Well, yes -- we can leverage the utility of automatic differentiation to trace each iteration of the optimization loop at runtime, and then do the corresponding gradient calculation by composing VJPs and the like. However, that could get really expensive as the number of iterations gets into the thousands, which isn't too uncommon in practice. Do we have a way to get around this?

Thanks to the groundwork we set up in @sec-fixed-points, we worked out that we can take the gradient of **fixed points** (e.g. solutions to minimization algorithms) through a simple analytic formula in terms of the update step $f$, the solution of the optimization problem $\theta_0$ (or $\hat{\theta}$), and some particular value of $\varphi=\varphi_0$ that we used to define the objective:

$$
\frac{\partial\theta_0}{\partial\varphi_0}= \left[I - \frac{\partial f}{\partial \theta_0} \right]^{-1} \frac{\partial f}{\partial \varphi_0}~.
$${#eq-fixed-point}

What does $\varphi$ mean here? It corresponds to the *same* $\varphi$ that we're talking about in this section (the notation was not a coincidence)! Specifically, these would be the *analysis configuration parameters* (e.g. a combination of neural network parameters, observable binning, cutflow, etc.), which all *implicitly* determine the form of the likelihood. The language of "implicit" refers to the fact that we build the likelihood using the counts of the histograms for each physics process, with those counts in turn being influenced by $\varphi$, but we do not explicitly denote the likelihood as $p(x|\mu, \theta, \varphi)$, for instance.

In practice, we can implement this through moving the goalposts for what we call a primitive: for optimization loops like this, we can define that as a primitive of sorts, and then give it the known gradient as defined by @eq-fixed-point. This is the kind of approach taken by `jaxopt` [@jaxopt], which is a library that's used a few times in this thesis.


### Differentiable hypothesis tests

What's left to get us over the line to differentiating the result of a hypothesis test? Well, thanks to the formulae outlined in @sec-asymptotics, to extract the expected (read: median) $p$-value from the observed value of the test statistic $t(x_0)$, we only need to do one last simple algebraic calculation:

- For the **discovery $p$-value** with test statistic $q_0$: $p_0 = 1-\Phi(\sqrt{q_0})$.
- For the $p$-value associated with setting an **upper limit** using test statistic $q_\mu$: $p_\mu = 1-\Phi(\sqrt{q_\mu})$.
- For the $\text{CL}_s$ **method**, we can just compose the $p$-values from the previous step using different values of $\mu$ in $q_\mu$ as the point null: $\text{CL}_s = p_{\mu=1} / (1-p_{\mu=0})$.

All of these formulae are differentiable without any extra work, so we're done!

### Bonus: Uncertainties on likelihood parameters {#sec-inferno}

Recall from @sec-fisher that the *Fisher information matrix* $\mathcal{I}(\theta)$ gives us access to the covariance matrix for maximum likelihood estimates, provided we're in the asymptotic limit, through the Cramér–Rao bound:

$$
\Sigma_\hat{\theta}^2 \geqslant [\mathcal{I}(\theta)]^{-1}~.
$$

Since $\mathcal{I}(\theta)$ is defined in terms of second-order derivatives of the log-likelihood -- something that we've already made differentiable from a code perspective -- we can then calculate the Fisher information using automatic differentiation. Moreover, since function transformations like the gradient operator are composable in automatic differentiation frameworks, this will itself be differentiable! This gives us access to many other inference-aware quantities that we can use as both diagnostics and as loss functions, including diagonal elements of the covariance matrix, which correspond to the individual uncertainties on each likelihood parameter. This approach was first explored by INFERNO [@inferno], from whom we take much inspiration from in this section.


## Putting it all together!

Now that we've enabled the differentiability of all the components layed out in @eq-neos, we can see what happens when we use use the $p$-value as out loss function, i.e. use gradients in @eq-analysis-chain-rule to update the parameters $\varphi$.

As a first example, we can look at the simple analysis from @sec-simple-anal as a candidate to test this out on! We were looking for the value of $\phi$ that corresponds to the best expected sensitivity to the signal model. Crucially, we needed to make sure that we accounted for the systematic uncertainty on the background $\sigma_b$, which heavily influenced the resulting optimization. Let's see if we can do that!

The results of training this setup to optimize $\phi$ with respect to the discovery $p$-value can be seen in @fig-simple-opt. Immediately, we can see that we've managed to find a point that minimizes the objective we care about, being able to incorporate uncertainty all the while! One thing I really like about this plot is the way it appeals to intuition -- the expected counts that result from this procedure appear to be exactly at some medium compromise between signal to background ratio and uncertainty on the background. The acquired Swede within me would even go as far as to call it "lagom" -- not too little, not too much; just the right amount^[If you're less of a holistic person and would prefer something more quantitative, you can see a real-life carving of the "lagom" amount in Lund, just outside one of the main university buildings.].

![Left: The resulting point from optimizing $\phi$ with gradient descent. Right: The histogram model at the solution of the optimization.](images/opt){#fig-simple-opt}

Equipped with this systematic-aware loss function, we can now apply it to something a little more complicated.

## `neos`: End-to-End Optimized Summary Statistics for High-Energy Physics

The problem of reducing a whole set of physics quantities into a single number, or **summary statistic**, is not a new one in HEP. The reason for this is two-fold: inference in high-dimensional settings is computationally difficult, and the probability model defined by the underlying physics is intractable to explicitly compute. This leads to practice referenced a few times already, where we construct models via HistFactory, which are almost always based a single quantity, e.g. something like the invariant mass of the final state you're interested in, or the output of a machine learning model. In the latter case, we're typically concerned with optimizing for discriminating signal and background processes. However, we know from the previous sections that we can do much better by optimizing with respect to a $p$-value, especially when we have significant systematic uncertainties to worry about.

To be more concrete: we'll look at optimizing a neural network-based summary statistic for a simple HEP-like situation, including the construction of the uncertainty through interpolating between "up" and "down" variations of the background. We'll refer to this workflow as `neos`^[Originally an acronym for neural end-to-end optimized statistics, but acronyms are annoying, so I don't really make that explicit anymore. Hopefully the rest of this section will also convince you that we don't need to get too caught up with naming anyway.], with the full proposed pipeline shown in @fig-neos.

![Outline of the workflow proposed for `neos`.](images/neos){#fig-neos}

### Example: Gaussian blobs

Pretending our detector only outputs two physics variables $x$ and $y$, we'll generate some toy data from different 2-D normal distributions ("Gaussian blobs") for both signal and background, making sure they overlap a bit as to not be trivially separable. We'll then also sample from Gaussian blobs on either side of the background, and treat these as "up" and "down" variations in the way described in @sec-hifa-nps. We can see the result of sampling 10k points for each blob in @fig-data-space.

![Plot of 10k samples from toy distributions representing imagined signal, background](images/data-space){#fig-data-space}

From here, things follow the diagram in @fig-neos:

- We'll pass the samples of $x, y$ values for each blob through a neural network, which turns each tuple of $(x, y)$ into a single number $f_\varphi(x, y)$, where $f_\varphi$ is our current network with parameters $\varphi$.
- We'll then have four sets of values of $f_\varphi$ for each blob (signal, nominal background, and the up and down variations of the background), which we then turn into four histograms. During optimization, this histogram will be a bKDE as per @sec-bkde to make the final loss ($\mathrm{CL}_s$) differentiable, but we'll replace it with a regular histogram for calculating our evaluation metrics.
- Using these histograms, we'll build a HistFactory statistical model in exactly the same fashion as @eq-simplemodel -- with one parameter for the signal strength $\mu$, and one nuisance parameter $\gamma$ that's constructed by interpolating the shape variation between the histograms of the nominal, up, and down variations of the background.
- We then build the appropriate test statistic based on this likelihood, and perform our hypothesis test -- not on the observed data (we don't have any), but on the Asimov dataset for that hypothesis (i.e. for a null of $\mu=1$, we're going to use the dataset that would result in $\hat{\mu} = 1$ when fitting the likelihood, which would just be the nominal counts $s + b$).
- The final step is producing the end result of that test (e.g. $\mathrm{CL}_s$), then taking the gradient of that whole chain, which we'd use to update the parameters of the neural network $\varphi$.

We'll feed data in using mini-batches of size 4000, and hold out a group of points for each blob as a test set, which we use to calculate metrics and select the best model (normally we should never use the test set to choose a model, but the distributions are simple enough that there will be almost no macroscopic difference between train, validation, and test sets).

In the graphs about to be shown, we'll benchmark `neos` against optimization against some other loss functions:

- **Binary cross-entropy** (BCE): This will try to discriminate signal versus nominal background samples, and will not be informed about the up/down samples during training.
- **BCE with data augmentation**: As above, but we indiscriminately label all of the background samples with one label instead of using just the nominal (this should be a very powerful baseline).
- **INFERNO**: As in @sec-inferno, we'll take our loss to be the diagonal element of the inverse Fisher information that corresponds to the signal strength $\mu$.

Results from the training process are shown in @fig-neos-results, where each curve is the average of that metric across 7 different training runs, all with unique random initializations of the neural network parameters. The figure has three plots, which we'll cover from left to right. Note: these quantities are all computed on the same *test set* (unseen data), and use no approximations to hard operations in their calculation (which basically means the histogram is a normal one for `neos` and INFERNO).

![Results from training neural networks using the four loss functions highlighted in the legend. Left: the expected $\text{CL}_s$. Middle: The uncertainty on the signal strength $\mu$. Right: The deviation from the nominal width of the nuisance parameter $\gamma$ that controls the background uncertainty.](images/neos-results){#fig-neos-results}

The leftmost plot contains the expected $\mathrm{CL}_s$ for all four methods. We can see that the absolute lowest value of this is only attained by `neos`, which makes sense -- we're using this as the loss function itself. Both BCE methods reach a fairly low value quite quickly, which can be attributed to the fact that it's not that difficult to isolate most of the signal in this example. Interestingly, INFERNO demonstrated far less stability with it's $\mathrm{CL}_s$ value, with the different training runs having quite large variance in this quantity, causing their average to perform quite poorly. This says more about the interplay between the uncertainty on the signal strength and the $\mathrm{CL}_s$ than it does about what we consider "good performance" -- I'll come back to this in the discussion.

The middle plot is the uncertainty on the fitted value of the signal strength $\sigma_{\hat{\mu}}$, as calculated through the Fisher information. Something I think is cool here is that `neos` learns to minimize this over time without any explicity prompting to do so (we'll discuss more on this later). It's no surprise that INFERNO does very well here, as again, it's trained precisely to do so. BCE with augmentation also does very well, which is likely because we've ensured that anything that is non-signal looking, including possible variations on the background, is pushed to one side of the histogram, and the signal, which can be well isolated, is pushed to the other. Regular BCE, however, does okay initially, but will quickly overfit to the nominal signal/background discrimination if left to train for longer, which won't care if there's high background uncertainty in the signal bins.

The final plot on the right shows the squared deviation from the nominal uncertainty on the background nuisance parameter $\gamma$ (which has uncertainty rescaled to 1 in the likelihood modelling stage), which we acquire through the appropriate diagonal term in the inverse Fisher information. When doing inference using a HistFactory likelihood, we don't expect the fitted value of $\gamma$ to have uncertainty that differs from 1, as this implies a bias away from the information we provided from the up/down variations. This is known as over/under-constraining the parameter (depending if the uncertainty is smaller/bigger than 1), and is generally associated with model misspecification. Here, this would mean that the corresponding observable that's been learned does not faithfully represent the background data that's being fed through the neural network and subsequently modelled. We plot $(1-\sigma_{\hat{\gamma}})^2$ so that all plots can be read as lower values implying a more performant network. Once again, `neos` is able to keep this reasonably low as it trains without any additional information. This is also shown in BCE with augmentation, which does very well in this regard, likely because the learned network will have a decision boundary that traces the border between the three blobs to separate them, which will result in bins that have a all the background and it's variations in at once. BCE is a little less good, and INFERNO is not able to keep this deviation small at all.


## Discussion




- 1 forward pass is one analysis
- mini-batch size needs to accurately reflect whole analysis to some degree -> big VRAM constraint


### What's the true objective really?

This all ultimately shapes lhood. Goals are related -- miminizing cls will maximise non-centrality parameter (think more about this), which is inv propto mu uncert (but may have better accuracy than fisher info estimate).

gaussianity experiment
